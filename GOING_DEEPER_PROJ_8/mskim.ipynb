{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **27. LLM Trend Note 2[프로젝트]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **27-1. 프로젝트: KoChatGPT 업그레이드 하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoChatGPT 소스코드를 바탕으로 다양한 모델 개선 전략을 선택해 KoChatGPT를 업그레이드해 보겠습니다.<br>\n",
    "\n",
    "아래 제시된 전략 중 하나를 선택하거나 여러 개를 조합하여\n",
    "여러분만의 custom ChatGPT를 개발해보세요. 물론 더 창의적인 좋은 아이디어를 도입해볼 수도 있겠죠?<br>\n",
    "\n",
    "복수의 전략을 선택했을 때 혼자서 실험해볼 시간이 부족하다면\n",
    "팀을 이뤄 분업을 해보셔도 좋습니다!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.우리가 지난시간 살펴본 KoChatGPT 모델에 사용한 데이터셋은 아직 완벽히 정제되지 않았습니다.\n",
    "\n",
    "> 2.Hunman Feedback이 반영된 데이터셋을 대체하기 위해\n",
    "SFT와 RM 모델에 사용할 다양한 benchmark 데이터셋도 검토해볼 수 있습니다.\n",
    "\n",
    "> 3.언어모델의 생성능력을 좌우하는 최선의 디코딩을 위한 하이퍼파라미터 서치가 필요합니다.\n",
    "\n",
    "> 4.생성된 답변에 대한 주관적인 평가를 보완할 수 있는 정량적인 메트릭은 도입하지 않았었습니다.\n",
    "\n",
    "> 5.LLM Trend Note1에서 살펴본 다양한 Instruction Tuning 및 Prompting 기법들도 적용해볼만 합니다.\n",
    "\n",
    "> 6.무엇보다 foundation model로 사용한 KoGPT-2는 Emergent abilities를 기대하기엔 다소 작은 사이즈의 모델입니다.\n",
    "더 큰 파라미터 스케일을 가진 모델을 사용해보거나,\n",
    "\n",
    "> 7.더 효율적인 연산을 수행할 수 있는 LoRA의 적용 또는\n",
    "새로운 Instruction Tuning 및 reward ranking 알고리즘을 도입해볼 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 걸 해야할 지 감이 잡히지 않는 분들을 위해\n",
    "몇 가지 예시를 소개해드리도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **기존 데이터셋 추가 정제**\n",
    "\n",
    "data_kochatgpt 폴더에는 세 파일이 있습니다.<br>\n",
    "ㄱ. kochatgpt_1_SFT.jsonl : SFT를 위한 prompt와 completion 문장셋<br>\n",
    "ㄴ. kochatgpt_1_RM.jsonl : RM 학습을 위한 prompt와 세 가지 ranking 문장셋<br>\n",
    "ㄷ. kochatgpt_1_PPO.jsonl : promt 문장<br>\n",
    "\n",
    "각 말뭉치를 EDA하여 도메인과 문체, 길이분포, 문장의 완성도 등을 분석합니다.<br>\n",
    "언어모델의 문장생성능력은 말뭉치의 전처리 수준에 큰 영향을 받습니다.<br>\n",
    "말뭉치의 분석결과를 토대로 데이터를 정제하여 모델을 재학습시켜봅니다.<br>\n",
    "(정제후 데이터셋 크기가 줄어들지 않도록, 다양한 augmentation 기법을 활용하여 크기를 유지 내지 증량합니다.)<br>\n",
    "추가 전처리 후, 기존 인퍼런스 결과와 성능을 비교해봅니다.<br>\n",
    "(주관적인 평가와 BLEU, ROUGE 등을 활용한 정량적인 평가 결과를 비교 분석하여 제시합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Sequence\n",
    "##\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora\n",
    "import random\n",
    "##\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SFT 데이터셋 확인\n",
    "data_path_1_SFT = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `SFT 데이터셋`은 `list_dict` 형태의 데이터 구조를 가진다.\n",
    "2. `list_dict` 형태의 `item`은 `prompt`와 `completion`을 각각 `Key`로 하는 `dict` 타입의 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          prompt  \\\n",
      "0                                  불고기용 고기 한우에요?   \n",
      "1                                 쓰던 앱이 유료로 전환됐어   \n",
      "2                                        여친이랑 다툼   \n",
      "3                                        술 먹고 싶어   \n",
      "4                                          잊고싶다.   \n",
      "...                                          ...   \n",
      "11995                  내일 개강총회인데 짝녀한테 어떻게 잘 보이지.   \n",
      "11996                                    역세권이에요?   \n",
      "11997  암호화폐를 분산 저장하여 운영 관리 될 수 있도록 하는 인터넷상의 방식은?   \n",
      "11998                               가출해도 갈 데가 없어   \n",
      "11999                                  목포극장 개업일은   \n",
      "\n",
      "                                              completion  tokens  \n",
      "0      '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...     193  \n",
      "1      '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...     288  \n",
      "2      '저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...     153  \n",
      "3      '술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...     189  \n",
      "4      '저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...     147  \n",
      "...                                                  ...     ...  \n",
      "11995  '제가 AI 모델이기 때문에 남성인점 참고 부탁드립니다. 다음과 같이 제안드립니다....     411  \n",
      "11996  \"저는 인공지능 언어모델이어서 지리적인 개념을 이해하는 데 한계가 있습니다. 반면,...     232  \n",
      "11997  '블록체인 기술을 사용하여 암호화폐를 분산 저장하여 운영 및 관리하는 것을 의미합니...     214  \n",
      "11998  '그렇다면, 우선적으로 생활에 필요한 것들을 생각해보시는 것이 좋습니다. 가장 먼저...     473  \n",
      "11999  '제가 알기로는 정확한 날짜를 알 수 없습니다. 재개발로 구조가 변경되고 리모델링 ...     120  \n",
      "\n",
      "[12000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 데이터를 Pandas 데이터프레임으로 변환\n",
    "df = pd.DataFrame(list_data_dict)\n",
    "\n",
    "# 데이터프레임 확인\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 히스토그램 그리기 (tokens)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['tokens'], bins=10, alpha=0.5, color='blue')\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'prompt'의 문장 길이 계산\n",
    "df['prompt_length'] = df['prompt'].apply(lambda x: len(x))\n",
    "\n",
    "# 히스토그램 그리기 (prompt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['prompt_length'], bins=10, alpha=0.5, color='green')\n",
    "plt.xlabel('Prompt Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'completion'의 문장 길이 계산\n",
    "df['completion_length'] = df['completion'].apply(lambda x: len(x))\n",
    "\n",
    "# 히스토그램 그리기 (prompt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['completion_length'], bins=10, alpha=0.5, color='red')\n",
    "plt.xlabel('Completion Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tokens  prompt_length  completion_length\n",
      "count  12000.000000   12000.000000       12000.000000\n",
      "mean     159.311667      22.180583         144.107250\n",
      "std      109.054326      14.110028         122.843692\n",
      "min       17.000000       0.000000           4.000000\n",
      "25%       85.000000      13.000000          62.000000\n",
      "50%      134.000000      19.000000         118.000000\n",
      "75%      198.250000      28.000000         185.000000\n",
      "max     1111.000000     295.000000        1553.000000\n"
     ]
    }
   ],
   "source": [
    "# 통계적 요약 정보\n",
    "summary_stats = df.describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# SFT를 처리할 모델과 토크나이저 불러오기\n",
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 inferrence에서 사용할 클래스 정의\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        data_path_1_SFT = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "        \n",
    "        # 여기다가 전처리를 해야겠네..\n",
    "        ## completion 길이별 전처리 (via MAX_LEN)\n",
    "        list_data_dict = check_sentence_length(list_data_dict, MAX_LEN=250)\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 지도학습용 데이터 전처리 클래스 선언\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT 데이터셋 전처리 함수 생성\n",
    "## completion의 길이를 체크하여 특정 길이 이상의 데이터를 없애는 로직으로 접근\n",
    "def check_sentence_length(list_dict, MAX_LEN=250):\n",
    "    res_list = []\n",
    "    \n",
    "    for item in list_dict:\n",
    "        if len(item['completion']) <= MAX_LEN:\n",
    "            res_list.append(item)\n",
    "    \n",
    "    return res_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 10576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터셋을 만들기\n",
    "train_dataset = SFT_dataset(data_path_1_SFT='/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : ### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\n",
      "output: \n"
     ]
    }
   ],
   "source": [
    "# 텐서를 텍스트로 디코딩하는 함수\n",
    "def decode_tensor_to_text(tokenizer, tensor):\n",
    "    # 텐서를 CPU로 이동하고, numpy 배열로 변환\n",
    "    tensor = tensor.cpu().numpy()\n",
    "\n",
    "    # 패딩 토큰과 -100을 제외한 토큰만 선택\n",
    "    tokens = []\n",
    "    for token in tensor:\n",
    "        if token == tokenizer.pad_token_id or token == -100:  # 패딩 토큰 또는 -100일 경우 루프 종료\n",
    "            break\n",
    "        tokens.append(token)\n",
    "\n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    decoded_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return decoded_text\n",
    "\n",
    "# input_ids와 labels 텐서를 디코딩하여 텍스트로 출력\n",
    "input_text = decode_tensor_to_text(tokenizer, train_dataset.input_ids[0])\n",
    "output_text = decode_tensor_to_text(tokenizer, train_dataset.labels[0])\n",
    "\n",
    "print('input :', input_text)\n",
    "print('output:', output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 Trainer 클래스 정의\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1322' max='1322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1322/1322 03:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.681300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SFT 훈련 진행\n",
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전처리 중 문장길이(`completion`의 길이)가 250 이하인 데이터만 학습데이터로 사용\n",
    "1. 동일 `step`에서 training loss가 조금 줄었다.\n",
    "    - 학습노드 기준 500 step loss : 2.984100\n",
    "    - 학습노드 기준 100 step loss : 2.776800\n",
    "    - 학습노드 기준 1500 step loss : 2.687200\n",
    "2. 데이터가 줄어서 그런지 1500 스텝이 훈련중에 보이지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 실제로 불고기용 고기를 구매할 수 없습니다. 하지만 일반적으로 불고기용 고기는 다양한 종류의 요리와 함께 먹을 수 있기 때문에 취향에 따라 선택하시면 좋을 것 같습니다. 감사합니다.辛酉)辛酉辛酉申酉申酉辛酉辛\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 41대 부통령직을 수행하지 않았습니다. Captain) Control: \"리처드 닉슨\" 또는 \"Captain Johnson\" 등의 용어가 사용됩니다. Controversion)은 리처드 닉슨이 47대 부통령직을 수행할 때 사용한 용어 중\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오헤이어 국제공항은 미국 일리노이주 시카고에 위치해 있습니다. Americano Pacific Airports의 약자입니다. Americano-Pacific Administrations의 약자로 \"시카고\" 또는 \"오헤어\"를 의미합니다. Americano\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이므로 미세먼지와 관련된 정보를 알 수 없습니다. 하지만 인터넷 검색을 통해 미세먼지 정보를 찾아보시는 것을 추천드립니다! 감사합니다.美濃)美濃相應: \"미세먼지\" 입니다.美濃上應:\n"
     ]
    }
   ],
   "source": [
    "# SFT 모델 검증 (문장 생성 능력 확인)\n",
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BLEU SCORE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def bleu_score(reference, candidate):\n",
    "    # 실제 번역된 문장 (모델의 출력)\n",
    "    reference = reference.split()\n",
    "    \n",
    "    # 모델이 생성한 번역 후보 문장\n",
    "    candidate = candidate.split()\n",
    "    \n",
    "    # BLUE 스코어 계산\n",
    "    bleu_score = sentence_bleu([reference], candidate)\n",
    "    \n",
    "    # 소수점 이하 4자리까지 출력\n",
    "    formatted_bleu_score = format(bleu_score, '.4f')\n",
    "    \n",
    "    return formatted_bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ROUGE SCORE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rough_socre_n1(reference, candidate):\n",
    "    \n",
    "    ## ROUGE 스코어 계산; ROUGE-1 (1-gram)\n",
    "    score_1 = rouge_scorer.RougeScorer(['rouge1'])\n",
    "    scores_1 = score_1.score(reference, candidate)\n",
    "    \n",
    "    \n",
    "\n",
    "    return scores_1\n",
    "\n",
    "def rough_socre_n2(reference, candidate):\n",
    "    ## ROUGE 스코어 계산; ROUGE-2 (2-gram)\n",
    "    score_2 = rouge_scorer.RougeScorer(['rouge2'])\n",
    "    scores_2 = score_2.score(reference, candidate)\n",
    "\n",
    "    return scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0645\n",
      "===\n",
      "ROUGE-1 Precision: 0.0\n",
      "ROUGE-1 Recall: 0.0\n",
      "ROUGE-1 F1 Score: 0.0\n",
      "===\n",
      "ROUGE-2 Precision: 0.0\n",
      "ROUGE-2 Recall: 0.0\n",
      "ROUGE-2 F1 Score: 0.0\n",
      "BLEU Score: 0.0000\n",
      "===\n",
      "ROUGE-1 Precision: 0.0\n",
      "ROUGE-1 Recall: 0.0\n",
      "ROUGE-1 F1 Score: 0.0\n",
      "===\n",
      "ROUGE-2 Precision: 0.0\n",
      "ROUGE-2 Recall: 0.0\n",
      "ROUGE-2 F1 Score: 0.0\n",
      "BLEU Score: 0.0000\n",
      "===\n",
      "ROUGE-1 Precision: 0.0\n",
      "ROUGE-1 Recall: 0.0\n",
      "ROUGE-1 F1 Score: 0.0\n",
      "===\n",
      "ROUGE-2 Precision: 0.0\n",
      "ROUGE-2 Recall: 0.0\n",
      "ROUGE-2 F1 Score: 0.0\n",
      "BLEU Score: 0.0000\n",
      "===\n",
      "ROUGE-1 Precision: 0.0\n",
      "ROUGE-1 Recall: 0.0\n",
      "ROUGE-1 F1 Score: 0.0\n",
      "===\n",
      "ROUGE-2 Precision: 0.0\n",
      "ROUGE-2 Recall: 0.0\n",
      "ROUGE-2 F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# 기존 데이터셋을 `### Instruction(명령어):`로 순회하여 얻은 답변을 기반으로 구성함\n",
    "candidates_list = [\n",
    "    \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\", \n",
    "    \"'1953년입니다.\", \n",
    "    \"''\", \n",
    "    \"'미세먼지 농도는 어제와 비교해서 개선되었지만 아직도 나쁜 수준이며, 마스크 착용과 실외 활동 자제를 권장합니다. 정확한 미세먼지 농도를 확인하려면 해당 지역의 미세먼지 측정소에서 확인해보시기 바랍니다.\"\n",
    "]\n",
    "# 평가하고 나온 응답을 기반으로 구성함\n",
    "reference_list = [\n",
    "    \"저는 인공지능 어시스턴트이기 때문에 실제로 불고기용 고기를 구매할 수 없습니다. 하지만 일반적으로 불고기용 고기는 다양한 종류의 요리와 함께 먹을 수 있기 때문에 취향에 따라 선택하시면 좋을 것 같습니다. 감사합니다.辛酉)辛酉辛酉申酉申酉辛酉辛\", \n",
    "    \"리처드 닉슨은 41대 부통령직을 수행하지 않았습니다.\", \n",
    "    '시카고 오헤이어 국제공항은 미국 일리노이주 시카고에 위치해 있습니다. Americano Pacific Airports의 약자입니다. Americano-Pacific Administrations의 약자로 \"시카고\" 또는 \"오헤어\"를 의미합니다. Americano', \n",
    "    '저는 인공지능 어시스턴트이므로 미세먼지와 관련된 정보를 알 수 없습니다. 하지만 인터넷 검색을 통해 미세먼지 정보를 찾아보시는 것을 추천드립니다! 감사합니다.美濃)美濃相應: \"미세먼지\" 입니다.美濃上應:'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(reference_list)):\n",
    "    print(f\"BLEU Score: {bleu_score(reference_list[i], candidates_list[i])}\")\n",
    "    print(\"===\")\n",
    "    print(f\"ROUGE-1 Precision: {rough_socre_n1(reference_list[i], candidates_list[i])['rouge1'].precision}\")\n",
    "    print(f\"ROUGE-1 Recall: {rough_socre_n1(reference_list[i], candidates_list[i])['rouge1'].recall}\")\n",
    "    print(f\"ROUGE-1 F1 Score: {rough_socre_n1(reference_list[i], candidates_list[i])['rouge1'].fmeasure}\")\n",
    "    print(\"===\")\n",
    "    print(f\"ROUGE-2 Precision: {rough_socre_n2(reference_list[i], candidates_list[i])['rouge2'].precision}\")\n",
    "    print(f\"ROUGE-2 Recall: {rough_socre_n2(reference_list[i], candidates_list[i])['rouge2'].recall}\")\n",
    "    print(f\"ROUGE-2 F1 Score: {rough_socre_n2(reference_list[i], candidates_list[i])['rouge2'].fmeasure}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ROUGE` 스코어에 사용된 문장이 이상해서 그런걸까?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CUDA 캐시 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RM 데이터셋 확인\n",
    "data_path_2_RM = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  prompt  \\\n",
      "0      번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...   \n",
      "1                                개포주공아파트는 몇 단지로 이루어져 있나?   \n",
      "2                     김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?   \n",
      "3                               페르시아로부터 기원된 아랍요리의 특징이 뭐야   \n",
      "4                         중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?   \n",
      "...                                                  ...   \n",
      "10215                           야세르 아라파트 국제공항은 어느 나라에 있어   \n",
      "10216                                          핸드폰이 잘 안돼   \n",
      "10217                                          토트백은 없나요?   \n",
      "10218                                               딱 좋아   \n",
      "10219                 아 네 선생님 서류 확인 하겠습니다. 차 번호 어떻게 됩니까?   \n",
      "\n",
      "                                            completion_0  \\\n",
      "0      Allow me to answer your question. I know that ...   \n",
      "1                             개포주공아파트는 다섯 단지로 이루어져 있습니다.   \n",
      "2      The diameter of the Metallic domain is bigger ...   \n",
      "3      1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...   \n",
      "4              중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.   \n",
      "...                                                  ...   \n",
      "10215  야세르 국제공항은 어느 나라에 있어\\n\\n야세르 국제공항은 이상 국가 폭태들 국가 ...   \n",
      "10216         가는 것 같았다.\\n\\nThey (the subject) said that.   \n",
      "10217  사용자들 개인개석을 하고, 토트백을 하고, 기별 개인개석을 하고, 기별 개인개석을 ...   \n",
      "10218  한다고한다\\n\\n다시 그런데 태민이 개인을 토려한 것 같다는 것도 가지다\\n\\n\\n...   \n",
      "10219  -아무래들 같은 것 같앗들 같앗 \\n\\n-다시 같은 것 같앗들 같앗 \\n\\n-다시 ...   \n",
      "\n",
      "                                            completion_1  \\\n",
      "0                   번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.   \n",
      "1                                          이날 목송에서 구글상위노   \n",
      "2      이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...   \n",
      "3      - 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...   \n",
      "4      The limit of the Office of the Vice President is?   \n",
      "...                                                  ...   \n",
      "10215        요?\\n\\n야세르 아라파트 국제공항은 아랍 에미리트 연합의 두바이에 있습니다.   \n",
      "10216  할 일:\\n\\n1. 배터리를 확인하세요. 배터리가 항상 충전되어 있지 않은 경우 핸...   \n",
      "10217                                 아직 제품이 출시되지 않았습니다.   \n",
      "10218                        그러게요! 좋아하면 사랑하는 마음으로 즐겨보세요!   \n",
      "10219                           내 차번호는 번호판에 적힌 것과 동일합니다.   \n",
      "\n",
      "                                            completion_2    ranking  \n",
      "0                                             라이언에게 말했다.  [2, 1, 0]  \n",
      "1                          개포주공아파트는 총 27개 단지로 이루어져 있습니다.  [2, 0, 1]  \n",
      "2      김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...  [1, 2, 0]  \n",
      "3      위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...  [0, 1, 2]  \n",
      "4               중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.  [0, 2, 1]  \n",
      "...                                                  ...        ...  \n",
      "10215                       야세르 아라파트 국제공항은 터키에 위치해 있습니다.  [2, 1, 0]  \n",
      "10216  요\\n\\n1. 먼저 핸드폰의 배터리를 꼭 확인해 보세요. 배터리가 부족한 경우 핸드...  [1, 2, 0]  \n",
      "10217  죄송합니다. 저는 AI 언어 모델로써 실시간으로 답변을 해드리고 있기 때문에, 어떤...  [2, 1, 0]  \n",
      "10218  딱 좋아요! 나는 이 노래를 정말 좋아합니다. 노래는 정말 좋은 멜로디와 가사로 이...  [1, 2, 0]  \n",
      "10219  죄송합니다. 저는 인공지능 어시스턴트이므로 차량 번호를 확인할 수 없습니다. 해당 ...  [2, 1, 0]  \n",
      "\n",
      "[10220 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 Pandas 데이터프레임으로 변환\n",
    "df = pd.DataFrame(list_data_dict)\n",
    "\n",
    "# 데이터프레임 확인\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "      <td>Allow me to answer your question. I know that ...</td>\n",
       "      <td>번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.</td>\n",
       "      <td>라이언에게 말했다.</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "      <td>개포주공아파트는 다섯 단지로 이루어져 있습니다.</td>\n",
       "      <td>이날 목송에서 구글상위노</td>\n",
       "      <td>개포주공아파트는 총 27개 단지로 이루어져 있습니다.</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "      <td>The diameter of the Metallic domain is bigger ...</td>\n",
       "      <td>이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...</td>\n",
       "      <td>김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "      <td>1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...</td>\n",
       "      <td>- 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...</td>\n",
       "      <td>위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.</td>\n",
       "      <td>The limit of the Office of the Vice President is?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.</td>\n",
       "      <td>[0, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...   \n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?   \n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?   \n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야   \n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?   \n",
       "\n",
       "                                        completion_0  \\\n",
       "0  Allow me to answer your question. I know that ...   \n",
       "1                         개포주공아파트는 다섯 단지로 이루어져 있습니다.   \n",
       "2  The diameter of the Metallic domain is bigger ...   \n",
       "3  1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...   \n",
       "4          중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.   \n",
       "\n",
       "                                        completion_1  \\\n",
       "0               번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.   \n",
       "1                                      이날 목송에서 구글상위노   \n",
       "2  이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...   \n",
       "3  - 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...   \n",
       "4  The limit of the Office of the Vice President is?   \n",
       "\n",
       "                                        completion_2    ranking  \n",
       "0                                         라이언에게 말했다.  [2, 1, 0]  \n",
       "1                      개포주공아파트는 총 27개 단지로 이루어져 있습니다.  [2, 0, 1]  \n",
       "2  김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...  [1, 2, 0]  \n",
       "3  위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...  [0, 1, 2]  \n",
       "4           중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.  [0, 2, 1]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGDCAYAAADDONJAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmHUlEQVR4nO3de7glVX3n//fHbi5ykYt0CDQgqD0YNIqkBZxoTES5xdhkxiB5Ymj9MekxQRMnyYx4mYAX5qf5jToyiRoMxIaggCgDMURsEU3ymwFsBJFLkJaLdMulobl6QZt8549aRzenz+mzG87u06fO+/U8+zlVq6pWrV2nuj+7Vq1TO1WFJEnqp6fNdAMkSdLoGPSSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEGvOSXJJ5L812mqa58kjyaZ1+a/muQ/TEfdrb5/SLJ0uurbhP2+P8l9Se7e3PvWxJKckuRvZ7odmp0MevVGktuT/DDJI0keTPK/k7w5yU/P86p6c1W9b8i6XrWxdarqu1W1Q1U9Pg1t3+A/8qo6qqqWP9W6N7Ed+wB/AhxQVT8/yTrvTHJb+5CzOsl507Tvaf2gNB2S/GqS1X3fp/rNoFff/EZV7Qg8C/gA8HbgjOneSZL5013nFmIf4P6quneiha2H4XeBV1XVDsBi4LLN2D5Jm8igVy9V1UNVdTHwemBpkhcAJPlUkve36d2SfKFd/a9L8k9JnpbkbLrA+7t21fpfkuybpJKckOS7wFcGygZD/zlJrkrycJKLkuza9rXBVdpYr0GSI4F3Aq9v+/tmW/7TK9zWrncnuSPJvUnOSrJTWzbWjqVJvtu63d812bFJslPbfm2r792t/lcBK4A9Wzs+NcHmLwEurarvtON8d1WdPq7uM5LclWRNuw0wdmvjjUn+Ocl/T/JA6xU4qi07FXg58Bdt33/Ryp+XZEX7/dyc5NiBfX0qyV8m+fvWi3NlkucMLH/+wLb3JHnnwLE8Kcl3ktyf5Pyx39OmSLJnks+143hbkj8cWHZKq/es1rYbkiweWH5Qkmvass8mOa8dq+2Bfxj4HTyaZM+22dYbqe/t7Xg/0o7TYZv6ftRfBr16raquAlbThch4f9KWLQB2pwvbqqrfBb5L1zuwQ1X9+cA2rwB+AThikl0eD/w/wB7AeuC0Idr4ReC/Aee1/b1ogtXe2F6/Bjwb2AH4i3HrvAzYHzgM+LMkvzDJLv8nsFOr5xWtzW+qqi8DRwHfa+144wTbXgEcn+Q/J1k8FuIDPkX3vp8LvBg4HBjsjj8EuBnYDfhz4Iwkqap3Af8EvKXt+y0t9FYAnwZ+DjgO+FiSAwbqOw54D7ALsAo4FSDJjsCXgS8Ce7b2jPU8vBU4pr33PYEHgL+c5FhNKN3toL8DvgkspDvmb0syeF68FjgX2Bm4mPb7SrI1cGE7VrsCnwF+E6Cqvs8Tfwc7VNX3pqhvf+AtwEtab9YRwO2b8n7Ubwa95oLv0f2HOt5P6AL5WVX1k6r6p5r6yx9OqarvV9UPJ1l+dlVd3/7D/q/AsROE4ZPxO8CHq+rWqnoUeAdw3LjehPdU1Q+r6pt0AbTBB4bWluOAd1TVI1V1O/Ahuu74KVXV39IF5RHA14B7k7y91b07cDTwtnaM7gU+0vY35o6q+mQb17Cc7vjvPsnuXgPcXlV/U1Xrq+oa4HPAbw2sc2FVXVVV64FzgAMHtr27qj5UVT9q7/XKtuzNwLuqanVVPQacArwum3Y75iXAgqp6b1X9uKpuBT457r3+c1Vd0t7r2fzs93EoMB84rZ13nweuGmKfk9X3OLANcECSrarq9rEeFwm6k03qu4XAugnK/z+6/+S/lATg9Kr6wBR13bkJy+8AtqK7en2q9mz1DdY9nyeG5OAo+R/QXfWPt1tr0/i6Fg7bkKo6BzgnyVZ0V8bnJLmW7sp4K+Cudjyhu5gYPCZ3D9Tzg7beRO2EbpzFIUkeHCibTxdyG9THE9/z3sBkYfcs4MIk/zpQ9jjdsVwzyTYT1bHnuLbNo+uVmKxt27YPE3sCa8Z9qJzqvJq0vqpaleRtdOfy85NcCvzxQE+A5jiv6NVrSV5CF2L/PH5Zu8r7k6p6Nl236B8P3Nuc7Mp+qiv+vQem96HrNbgP+D6w3UC75tHdMhi23u/Rhctg3euBe6bYbrz7WpvG1zVswP1Uuxr9LHAd8AK6sHoM2K2qdm6vZ1TV84etctz8ncDXBurauXVl//4Qdd1Jd2tismVHjat326ralGNwJ3DbuDp2rKqjh9j2LmBhBj4N8cTzZpO/UrSqPl1VL6P7vRbwwU2tQ/1l0KuXkjwjyWvo7mn+bVV9a4J1XpPkue0/3IforurGrvLuYfKg2Jg3JDkgyXbAe4ELWlfrt+muwH69XQm/m667dcw9wL4Z+FPAcT4D/Kck+yXZgZ/d01+/KY1rbTkfODXJjkmeBfwxMNTfaLcBdb/etn1aG0z3fODKqroL+BLwoXb8n5bkOUleMWTzxh/zLwD/JsnvJtmqvV6ykbEHg74A7JHkbUm2ae09pC37RHv/z2rvaUGSJVO8720HX3Rd7Y+0QXBPTzIvyQvaB8up/B+6c+0tSea3fR887jg8M22w5VSS7J/klUm2AX4E/JCfnceSQa/e+bskj9Bdcb0L+DDwpknWXUQ3YOtRuv98P1ZVl7dl/y/w7nQj8v90E/Z/Nt0gq7uBbYE/hO6vAIA/AP6a7ur5+3QDAcd8tv28P8k3Jqj3zFb3PwK30f2H/tZNaNegt7b930rX0/HpVv8wHqYbtPhd4EG6AXW/X1VjPSbHA1sDN9J15V9Adx9+GB+lu1f+QJLTquoRusF8x9H1aNxNd6W6zUbqALreGuDVwG+07W6hG8g4tp+L6W7ZPEI3wPCQieppFtKF5+BrP7pxAAfS/T7uo/vdThnOVfVj4N8BJ9AdwzfQfTB5rC3/F7oPdre282/PSaoasw3dn5Le197rz9GN4ZAAyNRjjyRJo5TkSuATVfU3M90W9Y9X9JK0mSV5RZKfb133S4EX0v0poDTtHHUvSZvf/nRjJbanu4XyujbGQZp2dt1LktRjdt1LktRjBr0kST3Wy3v0u+22W+27774z3QxJkjabq6+++r6qWjC+vJdBv++++7Jy5cqZboYkSZtNkjsmKrfrXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeqxkQZ9kv+U5IYk1yf5TJJtk+yX5Mokq5Kcl2Trtu42bX5VW77vQD3vaOU3JzlilG2WJKlPRhb0SRYCfwgsrqoXAPOA44APAh+pqucCDwAntE1OAB5o5R9p65HkgLbd84EjgY8lmTeqdkuS1Cej7rqfDzw9yXxgO+Au4JXABW35cuCYNr2kzdOWH5Ykrfzcqnqsqm4DVgEHj7jdkiT1wsi+va6q1iT578B3gR8CXwKuBh6sqvVttdXAwja9ELizbbs+yUPAM1v5FQNVD27zU0mWAcsA9tlnn2l/P6ecMu1VTqstvX2SpJkxyq77XeiuxvcD9gS2p+t6H4mqOr2qFlfV4gULNvg6XkmS5qRRdt2/CritqtZW1U+AzwO/DOzcuvIB9gLWtOk1wN4AbflOwP2D5RNsI0mSNmKUQf9d4NAk27V77YcBNwKXA69r6ywFLmrTF7d52vKvVFW18uPaqPz9gEXAVSNstyRJvTHKe/RXJrkA+AawHrgGOB34e+DcJO9vZWe0Tc4Azk6yClhHN9Keqrohyfl0HxLWAydW1eOjarckSX0ysqAHqKqTgZPHFd/KBKPmq+pHwG9NUs+pwKnT3kBJknrOJ+NJktRjBr0kST1m0EuS1GMGvSRJPWbQS5LUYwa9JEk9ZtBLktRjBr0kST1m0EuS1GMGvSRJPWbQS5LUYwa9JEk9ZtBLktRjBr0kST1m0EuS1GMGvSRJPWbQS5LUYwa9JEk9ZtBLktRjBr0kST1m0EuS1GMGvSRJPWbQS5LUYwa9JEk9ZtBLktRjBr0kST1m0EuS1GMGvSRJPWbQS5LUYwa9JEk9NrKgT7J/kmsHXg8neVuSXZOsSHJL+7lLWz9JTkuyKsl1SQ4aqGtpW/+WJEtH1WZJkvpmZEFfVTdX1YFVdSDwS8APgAuBk4DLqmoRcFmbBzgKWNRey4CPAyTZFTgZOAQ4GDh57MOBJEnauM3VdX8Y8J2qugNYAixv5cuBY9r0EuCs6lwB7JxkD+AIYEVVrauqB4AVwJGbqd2SJM1qmyvojwM+06Z3r6q72vTdwO5teiFw58A2q1vZZOVPkGRZkpVJVq5du3Y62y5J0qw18qBPsjXwWuCz45dVVQE1HfupqtOranFVLV6wYMF0VClJ0qy3Oa7ojwK+UVX3tPl7Wpc87ee9rXwNsPfAdnu1ssnKJUnSFDZH0P82P+u2B7gYGBs5vxS4aKD8+Db6/lDgodbFfylweJJd2iC8w1uZJEmawvxRVp5ke+DVwH8cKP4AcH6SE4A7gGNb+SXA0cAquhH6bwKoqnVJ3gd8va333qpaN8p2S5LUFyMN+qr6PvDMcWX3043CH79uASdOUs+ZwJmjaKMkSX3mk/EkSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqsZEGfZKdk1yQ5F+S3JTkpUl2TbIiyS3t5y5t3SQ5LcmqJNclOWignqVt/VuSLB1lmyVJ6pNRX9F/FPhiVT0PeBFwE3AScFlVLQIua/MARwGL2msZ8HGAJLsCJwOHAAcDJ499OJAkSRs3sqBPshPwK8AZAFX146p6EFgCLG+rLQeOadNLgLOqcwWwc5I9gCOAFVW1rqoeAFYAR46q3ZIk9ckor+j3A9YCf5PkmiR/nWR7YPeququtczewe5teCNw5sP3qVjZZuSRJmsIog34+cBDw8ap6MfB9ftZND0BVFVDTsbMky5KsTLJy7dq101GlJEmz3iiDfjWwuqqubPMX0AX/Pa1Lnvbz3rZ8DbD3wPZ7tbLJyp+gqk6vqsVVtXjBggXT+kYkSZqtRhb0VXU3cGeS/VvRYcCNwMXA2Mj5pcBFbfpi4Pg2+v5Q4KHWxX8pcHiSXdogvMNbmSRJmsL8Edf/VuCcJFsDtwJvovtwcX6SE4A7gGPbupcARwOrgB+0damqdUneB3y9rffeqlo34nZLktQLIw36qroWWDzBosMmWLeAEyep50zgzGltnCRJc4BPxpMkqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknpspEGf5PYk30pybZKVrWzXJCuS3NJ+7tLKk+S0JKuSXJfkoIF6lrb1b0mydJRtliSpTzbHFf2vVdWBVbW4zZ8EXFZVi4DL2jzAUcCi9loGfBy6DwbAycAhwMHAyWMfDiRJ0sbNRNf9EmB5m14OHDNQflZ1rgB2TrIHcASwoqrWVdUDwArgyM3cZkmSZqVRB30BX0pydZJlrWz3qrqrTd8N7N6mFwJ3Dmy7upVNVi5JkqYwf8T1v6yq1iT5OWBFkn8ZXFhVlaSmY0ftg8QygH322Wc6qpQkadYb6RV9Va1pP+8FLqS7x35P65Kn/by3rb4G2Htg871a2WTl4/d1elUtrqrFCxYsmO63IknSrDSyoE+yfZIdx6aBw4HrgYuBsZHzS4GL2vTFwPFt9P2hwEOti/9S4PAku7RBeIe3MkmSNIVRdt3vDlyYZGw/n66qLyb5OnB+khOAO4Bj2/qXAEcDq4AfAG8CqKp1Sd4HfL2t996qWjfCdkuS1BsjC/qquhV40QTl9wOHTVBewImT1HUmcOZ0t1GSpL7zyXiSJPWYQS9JUo8NFfRJfnHUDZEkSdNv2Cv6jyW5KskfJNlppC2SJEnTZqigr6qXA79D9/fsVyf5dJJXj7RlkiTpKRv6Hn1V3QK8G3g78ArgtCT/kuTfjapxkiTpqRn2Hv0Lk3wEuAl4JfAbVfULbfojI2yfJEl6Cob9O/r/Cfw18M6q+uFYYVV9L8m7R9IySZL0lA0b9L8O/LCqHgdI8jRg26r6QVWdPbLWSZKkp2TYe/RfBp4+ML9dK5MkSVuwYYN+26p6dGymTW83miZJkqTpMmzQfz/JQWMzSX4J+OFG1pckSVuAYe/Rvw34bJLvAQF+Hnj9qBolSZKmx1BBX1VfT/I8YP9WdHNV/WR0zZIkSdNhU76m9iXAvm2bg5JQVWeNpFWSJGlaDBX0Sc4GngNcCzzeigsw6CVJ2oINe0W/GDigqmqUjZEkSdNr2FH319MNwJMkSbPIsFf0uwE3JrkKeGyssKpeO5JWSZKkaTFs0J8yykZIkqTRGPbP676W5FnAoqr6cpLtgHmjbZokSXqqhv2a2t8DLgD+qhUtBP7XiNokSZKmybCD8U4Efhl4GKCqbgF+blSNkiRJ02PYoH+sqn48NpNkPt3f0UuSpC3YsEH/tSTvBJ6e5NXAZ4G/G12zJEnSdBg26E8C1gLfAv4jcAnw7lE1SpIkTY9hR93/K/DJ9pIkSbPEsM+6v40J7slX1bOnvUWSJGnabMqz7sdsC/wWsOv0N0eSJE2noe7RV9X9A681VfU/gF8fbdMkSdJTNewDcw4aeC1O8maG7/afl+SaJF9o8/sluTLJqiTnJdm6lW/T5le15fsO1PGOVn5zkiM2/W1KkjQ3Ddt1/6GB6fXA7cCxQ277R8BNwDPa/AeBj1TVuUk+AZwAfLz9fKCqnpvkuLbe65McABwHPB/YE/hykn9TVY8PuX9JkuasYbvuf23g9eqq+r2qunmq7ZLsRdfF/9dtPsAr6R6nC7AcOKZNL2nztOWHtfWXAOdW1WNVdRuwCjh4qHcnSdIcN2z3+x9vbHlVfXiSRf8D+C/Ajm3+mcCDVbW+za+me24+7eedrb71SR5q6y8Erhioc3AbSZK0EcM+MGcx8Pt0AbsQeDNwEF2A7zjRBkleA9xbVVdPQzunlGRZkpVJVq5du3Zz7FKSpC3esPfo9wIOqqpHAJKcAvx9Vb1hI9v8MvDaJEfT/UneM4CPAjsnmd+u6vcC1rT11wB7A6vbs/R3Au4fKB9syxrGqarTgdMBFi9e7HP4JUli+Cv63YEfD8z/uJVNqqreUVV7VdW+dIPpvlJVvwNcDryurbYUuKhNX9zmacu/UlXVyo9ro/L3AxYBVw3ZbkmS5rRhr+jPAq5KcmGbP4afDZzbVG8Hzk3yfuAa4IxWfgZwdpJVwDq6DwdU1Q1JzgdupBvxf6Ij7iVJGs6wz7o/Nck/AC9vRW+qqmuG3UlVfRX4apu+lQlGzVfVj+ieuDfh/oFTh92fJEnqDNt1D7Ad8HBVfZTuPvp+I2qTJEmaJsM+Ge9kui73d7SirYC/HVWjJEnS9Bj2iv43gdcC3weoqu8xyZ/VSZKkLcewQf/jNgK+AJJsP7omSZKk6TJs0J+f5K/o/gb+94AvA58cXbMkSdJ0mHLUfXve/HnA84CHgf2BP6uqFSNumyRJeoqmDPqqqiSXVNUvAoa7JEmzyLBd999I8pKRtkSSJE27YZ+MdwjwhiS30428D93F/gtH1TBJkvTUbTTok+xTVd8FjthM7ZEkSdNoqiv6/0X3rXV3JPlcVf37zdAmSZI0Taa6R5+B6WePsiGSJGn6TRX0Ncm0JEmaBabqun9Rkofpruyf3qbhZ4PxnjHS1kmSpKdko0FfVfM2V0MkSdL025SvqZUkSbOMQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPGfSSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPGfSSJPWYQS9JUo8Z9JIk9djIgj7JtkmuSvLNJDckeU8r3y/JlUlWJTkvydatfJs2v6ot33egrne08puTHDGqNkuS1DejvKJ/DHhlVb0IOBA4MsmhwAeBj1TVc4EHgBPa+icAD7Tyj7T1SHIAcBzwfOBI4GNJ5o2w3ZIk9cbIgr46j7bZrdqrgFcCF7Ty5cAxbXpJm6ctPyxJWvm5VfVYVd0GrAIOHlW7JUnqk5Heo08yL8m1wL3ACuA7wINVtb6tshpY2KYXAncCtOUPAc8cLJ9gm8F9LUuyMsnKtWvXjuDdSJI0+4w06Kvq8ao6ENiL7ir8eSPc1+lVtbiqFi9YsGBUu5EkaVbZLKPuq+pB4HLgpcDOSea3RXsBa9r0GmBvgLZ8J+D+wfIJtpEkSRsxylH3C5Ls3KafDrwauIku8F/XVlsKXNSmL27ztOVfqapq5ce1Ufn7AYuAq0bVbkmS+mT+1Ks8aXsAy9sI+acB51fVF5LcCJyb5P3ANcAZbf0zgLOTrALW0Y20p6puSHI+cCOwHjixqh4fYbslSeqNkQV9VV0HvHiC8luZYNR8Vf0I+K1J6joVOHW62yhJUt/5ZDxJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHDHpJknrMoJckqccMekmSesyglySpxwx6SZJ6zKCXJKnHRhb0SfZOcnmSG5PckOSPWvmuSVYkuaX93KWVJ8lpSVYluS7JQQN1LW3r35Jk6ajaLElS34zyin498CdVdQBwKHBikgOAk4DLqmoRcFmbBzgKWNRey4CPQ/fBADgZOAQ4GDh57MOBJEnauPmjqriq7gLuatOPJLkJWAgsAX61rbYc+Crw9lZ+VlUVcEWSnZPs0dZdUVXrAJKsAI4EPjOqts9Gp5wy0y2Y2mxooyT1zWa5R59kX+DFwJXA7u1DAMDdwO5teiFw58Bmq1vZZOXj97EsycokK9euXTu9b0CSpFlq5EGfZAfgc8DbqurhwWXt6r2mYz9VdXpVLa6qxQsWLJiOKiVJmvVGGvRJtqIL+XOq6vOt+J7WJU/7eW8rXwPsPbD5Xq1ssnJJkjSFUY66D3AGcFNVfXhg0cXA2Mj5pcBFA+XHt9H3hwIPtS7+S4HDk+zSBuEd3sokSdIURjYYD/hl4HeBbyW5tpW9E/gAcH6SE4A7gGPbskuAo4FVwA+ANwFU1bok7wO+3tZ779jAPEmStHGjHHX/z0AmWXzYBOsXcOIkdZ0JnDl9rZMkaW7wyXiSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPGfSSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPGfSSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPGfSSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPGfSSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPjSzok5yZ5N4k1w+U7ZpkRZJb2s9dWnmSnJZkVZLrkhw0sM3Stv4tSZaOqr2SJPXRKK/oPwUcOa7sJOCyqloEXNbmAY4CFrXXMuDj0H0wAE4GDgEOBk4e+3AgSZKmNrKgr6p/BNaNK14CLG/Ty4FjBsrPqs4VwM5J9gCOAFZU1bqqegBYwYYfHiRJ0iQ29z363avqrjZ9N7B7m14I3Dmw3upWNln5BpIsS7Iyycq1a9dOb6slSZqlZmwwXlUVUNNY3+lVtbiqFi9YsGC6qpUkaVbb3EF/T+uSp/28t5WvAfYeWG+vVjZZuSRJGsLmDvqLgbGR80uBiwbKj2+j7w8FHmpd/JcChyfZpQ3CO7yVSZKkIcwfVcVJPgP8KrBbktV0o+c/AJyf5ATgDuDYtvolwNHAKuAHwJsAqmpdkvcBX2/rvbeqxg/wkyRJkxhZ0FfVb0+y6LAJ1i3gxEnqORM4cxqbJknSnOGT8SRJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqMYNekqQeM+glSeoxg16SpB4z6CVJ6jGDXpKkHjPoJUnqsfkz3QDNHaecMtMt2LgtvX2S9GR4RS9JUo/NmqBPcmSSm5OsSnLSTLdHkqTZYFYEfZJ5wF8CRwEHAL+d5ICZbZUkSVu+WRH0wMHAqqq6tap+DJwLLJnhNkmStMWbLYPxFgJ3DsyvBg6ZobaopxyMNz08jtKWZbYE/ZSSLAOWtdlHk9w8zbvYDbhvmuuczTweG/KYAO95zxNmPSYb8phsyGPyRE/2eDxrosLZEvRrgL0H5vdqZT9VVacDp4+qAUlWVtXiUdU/23g8NuQx2ZDHZEMekw15TJ5ouo/HbLlH/3VgUZL9kmwNHAdcPMNtkiRpizcrruiran2StwCXAvOAM6vqhhluliRJW7xZEfQAVXUJcMkMNmFktwVmKY/HhjwmG/KYbMhjsiGPyRNN6/FIVU1nfZIkaQsyW+7RS5KkJ8Ggn4KP3u0kuT3Jt5Jcm2RlK9s1yYokt7Sfu8x0O0cpyZlJ7k1y/UDZhMcgndPaeXNdkoNmruWjM8kxOSXJmnauXJvk6IFl72jH5OYkR8xMq0cnyd5JLk9yY5IbkvxRK5+z58lGjslcPk+2TXJVkm+2Y/KeVr5fkivbez+vDT4nyTZtflVbvu8m7bCqfE3yohv49x3g2cDWwDeBA2a6XTN0LG4HdhtX9ufASW36JOCDM93OER+DXwEOAq6f6hgARwP/AAQ4FLhyptu/GY/JKcCfTrDuAe3f0DbAfu3f1ryZfg/TfDz2AA5q0zsC327ve86eJxs5JnP5PAmwQ5veCriy/f7PB45r5Z8Afr9N/wHwiTZ9HHDepuzPK/qN89G7G7cEWN6mlwPHzFxTRq+q/hFYN654smOwBDirOlcAOyfZY7M0dDOa5JhMZglwblU9VlW3Aavo/o31RlXdVVXfaNOPADfRPdlzzp4nGzkmk5kL50lV1aNtdqv2KuCVwAWtfPx5Mnb+XAAcliTD7s+g37iJHr27sRO0zwr4UpKr21MIAXavqrva9N3A7jPTtBk12TGY6+fOW1pX9JkDt3Tm1DFp3asvprta8zxhg2MCc/g8STIvybXAvcAKup6LB6tqfVtl8H3/9Ji05Q8Bzxx2Xwa9hvWyqjqI7hsET0zyK4MLq+tTmtN/wuEx+KmPA88BDgTuAj40o62ZAUl2AD4HvK2qHh5cNlfPkwmOyZw+T6rq8ao6kO5JrwcDzxvVvgz6jZvy0btzRVWtaT/vBS6kOzHvGetmbD/vnbkWzpjJjsGcPXeq6p72n9i/Ap/kZ92uc+KYJNmKLtDOqarPt+I5fZ5MdEzm+nkypqoeBC4HXkp362bs+TaD7/unx6Qt3wm4f9h9GPQb56N3gSTbJ9lxbBo4HLie7lgsbastBS6amRbOqMmOwcXA8W1U9aHAQwNdt7027h7zb9KdK9Adk+PaCOL9gEXAVZu7faPU7pueAdxUVR8eWDRnz5PJjskcP08WJNm5TT8deDXd2IXLgde11cafJ2Pnz+uAr7SeoeHM9OjDLf1FNyr223T3T9410+2ZoWPwbLpRsN8Ebhg7DnT3iC4DbgG+DOw6020d8XH4DF0X40/o7p+dMNkxoBtV+5ftvPkWsHim278Zj8nZ7T1f1/6D2mNg/Xe1Y3IzcNRMt38Ex+NldN3y1wHXttfRc/k82cgxmcvnyQuBa9p7vx74s1b+bLoPNauAzwLbtPJt2/yqtvzZm7I/n4wnSVKP2XUvSVKPGfSSJPWYQS9JUo8Z9JIk9ZhBL0lSjxn00iyR5F3tm66ua9/2dciTrOfAwW8K25yS7Dv4TXcj2sc7N+f+pC2dQS/NAkleCryG7lvAXgi8iic+D3xTHEj3d8x99c6pV5HmDoNemh32AO6rqscAquq+qvoeQJJfSvK19oVDlw48avWrST7Yvvf620le3p7w+F7g9a1X4PXtyYdntvWuSbKkbf/GJJ9P8sV036P+52ONSXJkkm+079O+rJVNWM8wNuU9tPLtkpyf7jvOL2zf0b04yQeAp7f3dk6rfl6ST7bekC+1J5FJc8dMPyHIly9fU7+AHeieKPZt4GPAK1r5VsD/Bha0+dcDZ7bprwIfatNHA19u028E/mKg7v8GvKFN79z2sX1b71a652pvC9xB97ztBXS9Cfu1bXbdWD3j3se+DHx3/VN4D38K/FWbfgGwnvZUOeDRcftbDxzY5s8fa6MvX3PlNfbwfElbsKp6NMkvAS8Hfg04L8lJwEq6oFvRvp56Ht0jaceMfanK1XShN5HDgdcm+dM2vy2wT5u+rKoeAkhyI/AsYBfgH6v7rnCqat0U9dw0xdvb/0m8h5cBH237vz7JdRup/7aqunaCOqQ5waCXZomqepzuCverSb5F9yUXVwM3VNVLJ9nssfbzcSb/9x7g31fVzU8o7Ab7PTZQtLE6Jq1nCOGpv4eNGf8e7LrXnOI9emkWSLJ/kkUDRQfSdaXfDCxog/VIslWS509R3SPAjgPzlwJvbd8yRpIXT7H9FcCvtG8WI8muT7KeMU/mPfz/wLFt/QOAXxxY9pN0X4sqCYNemi12AJa3wWfXAQcAp1TVj+m+tvKDSb5Jdx//305R1+XAAWOD8YD30d0nvy7JDW1+UlW1FlgGfL7t87y2aNh69k+yeuwFLHkS7+FjdB8ObgTeT/etig+1Zae3Npwz2cbSXOK310madZLMA7aqqh8leQ7dV7/u3z74SBrgPXpJs9F2wOWtiz7AHxjy0sS8opckqce8Ry9JUo8Z9JIk9ZhBL0lSjxn0kiT1mEEvSVKPGfSSJPXY/wWc674LdKZaFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'prompt'의 문장 길이 계산\n",
    "df['prompt_length'] = df['prompt'].apply(lambda x: len(x))\n",
    "\n",
    "# 히스토그램 그리기 (prompt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['prompt_length'], bins=10, alpha=0.5, color='blue')\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAGDCAYAAACydsMvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAif0lEQVR4nO3deZhtVX3m8e8rIIiogNwQJgWVxqAxilfBdkpEmRwwiQP9RL0aOnTikBi1Iw4dcEqrHTUSp6AQAVFB1IhGRUTU2GlBEETAIFcGuchwEWRwAC/59R97lR7KqlvnXu6pomp9P89zntp77X3WXnvXqTrvWXudvVNVSJKk/txtoRsgSZIWhiFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJCDJB5L8rw1U1/2S3JJkozb/1ST/fUPU3er7QpIVG6q+ddjum5Ncl+Tq+d62Zpbk8CQfWeh2aPEyBGjJS3JZkp8nuTnJT5L8e5I/T/Kr139V/XlVvWnMup68tnWq6odVtUVV3b4B2v4b/+Srav+qOubO1r2O7bgf8Epg96r67VnWeW2SS1sAWpXkhA207Q0aojaEJL+fZNVS36aWPkOAevH0qroXcH/grcCrgaM29EaSbLyh67yLuB/w46q6dqaFrWfi+cCTq2oLYDlw2jy2T9J6MASoK1V1Y1WdDDwXWJHkoQBJPpzkzW16mySfa70G1yf5tyR3S3Icw5vhZ9un3b9JsnOSSnJwkh8CXxkpGw0ED0xyZpKbknwmydZtW7/x6W6qtyHJfsBrgee27X2nLf/VJ+PWrtcnuTzJtUmOTXKftmyqHSuS/LB15b9utmOT5D7t+atbfa9v9T8ZOBXYvrXjwzM8/VHAKVX1g3acr66qI6fVfVSSq5Jc2U4tTJ0ueWGSbyT5+yQ3tN6E/duytwCPB97Ttv2eVv7gJKe2389FSZ4zsq0PJ3lvkn9tvT9nJHngyPKHjDz3miSvHTmWhyb5QZIfJzlx6ve0LpJsn+ST7ThemuQvR5Yd3uo9trXtgiTLR5bvkeSctuwTSU5ox+qewBdGfge3JNm+Pe3ua6nv1e1439yO097ruj9a2gwB6lJVnQmsYniDme6VbdkyYFuGN+KqqucDP2ToVdiiqt4+8pwnAr8D7DvLJl8A/CmwHbAGOGKMNn4R+DvghLa935thtRe2xx8ADwC2AN4zbZ3HAbsBewN/m+R3ZtnkPwL3afU8sbX5RVX1ZWB/4EetHS+c4bnfBF6Q5H8mWT71Bj/iwwz7/SDgEcA+wGgX/57ARcA2wNuBo5Kkql4H/Bvw0rbtl7Y3xFOBjwK/BRwEvC/J7iP1HQS8AdgKWAm8BSDJvYAvA18Etm/tmeqxeBnwzLbv2wM3AO+d5VjNKMMpps8C3wF2YDjmL08y+rp4BvBxYEvgZNrvK8ndgU+3Y7U18DHgDwGq6qfc8XewRVX9aI76dgNeCjyq9YLtC1y2Lvujpc8QoJ79iOGf7XS/ZHizvn9V/bKq/q3mvsnG4VX106r6+SzLj6uq89s/8/8FPGeGN8r18SfAO6vqkqq6BXgNcNC0Xog3VNXPq+o7DG9OvxEmWlsOAl5TVTdX1WXAOxi6+OdUVR9heBPdF/gacG2SV7e6twUOAF7ejtG1wLva9qZcXlUfbOMojmE4/tvOsrmnAZdV1T9X1ZqqOgf4JPDskXU+XVVnVtUa4Hjg4SPPvbqq3lFVv2j7ekZb9ufA66pqVVXdChwOPCvrdornUcCyqnpjVd1WVZcAH5y2r9+oqs+3fT2OX/8+9gI2Bo5or7tPAWeOsc3Z6rsd2BTYPckmVXXZVE+NNGWpnr+UxrEDcP0M5f+H4Q3gS0kAjqyqt85R1xXrsPxyYBOGT7131vatvtG6N+aOb6Cjo/l/xtBbMN02rU3T69ph3IZU1fHA8Uk2YfhEfXyScxk+UW8CXNWOJwwfQEaPydUj9fysrTdTO2EY17Fnkp+MlG3M8Ab4G/Vxx33eCZjtjfD+wKeT/OdI2e0Mx/LKWZ4zUx3bT2vbRgy9GbO1bbMWNLYHrpwWOOd6Xc1aX1WtTPJyhtfyQ5KcArxipAdBsidAfUryKIY3uG9MX9Y+Hb6yqh7A0NX6ipFzqbP1CMzVU7DTyPT9GHobrgN+Cmw+0q6NGE5DjFvvjxjeeEbrXgNcM8fzpruutWl6XeO++f1K+xT7CeA84KEMb2S3AttU1Zbtce+qesi4VU6bvwL42khdW7bu8b8Yo64rGE53zLZs/2n1blZV63IMrgAunVbHvarqgDGeexWwQ0aSEnd83azzLV+r6qNV9TiG32sBb1vXOrS0GQLUlST3TvI0hnOoH6mq786wztOSPKj9M76R4dPg1KfDa5j9TWRtnpdk9ySbA28ETmrdt99n+OT21PYJ+vUMXbhTrgF2zsjXGaf5GPDXSXZJsgW/HkOwZl0a19pyIvCWJPdKcn/gFcBY30Fvg/ue2p57tzaw7yHAGVV1FfAl4B3t+N8tyQOTPHHM5k0/5p8D/kuS5yfZpD0etZaxDqM+B2yX5OVJNm3t3bMt+0Db//u3fVqW5MA59nuz0QdD9/3NbUDePZJslOShLXTO5f8xvNZemmTjtu1HTzsO900b+DmXJLsleVKSTYFfAD/n169jCTAEqB+fTXIzwye11wHvBF40y7q7Mgweu4XhH/P7qur0tux/A6/P8M2BV63D9o9jGPB1NbAZ8JcwfFsBeDHwIYZP3T9lGJQ45RPt54+TfHuGeo9udX8duJThn/3L1qFdo17Wtn8JQw/JR1v947iJYQDlD4GfMAzu+4uqmuppeQFwd+BChtMDJzGc9x/HuxnOzd+Q5IiquplhYOFBDD0hVzN8wt10LXUAQy8P8BTg6e15FzMMqpzazskMp4FuZhjsuOdM9TQ7MLyxjj52YRh38HCG38d1DL/bOd+4q+o24I+AgxmO4fMYQsutbfl/MIS+S9rrb/tZqpqyKcPXYa9r+/pbDGNGpF/J3OOdJEkLIckZwAeq6p8Xui1amuwJkKS7iCRPTPLb7XTACuBhDF9nlCbCbwdI0l3HbgxjM+7JcFrmWW1MhTQRng6QJKlTng6QJKlThgBJkjrV3ZiAbbbZpnbeeeeFboYkSfPi7LPPvq6qls20rLsQsPPOO3PWWWctdDMkSZoXSS6fbZmnAyRJ6tTEQkCSozPc3/z8kbKtM9zH++L2c6tWniRHJFmZ5Lwke4w8Z0Vb/+L2vdmp8kcm+W57zhHTrrctSZLmMMmegA8D+00rOxQ4rap2ZbiH96GtfH+GS7XuChwCvB+G0AAcxnDpzkcDh00Fh7bOn408b/q2JEnSWkwsBFTV1/nN27QeyHCvcNrPZ46UH1uDbwJbJtmO4d7kp1bV9VV1A3AqsF9bdu+q+ma77eaxI3VJkqQxzPeYgG1Hrn51Nb++5/kO3PG+2ata2drKV81QPqMkhyQ5K8lZq1evvnN7IEnSErFgAwPbJ/h5uVxhVR1ZVcuravmyZTN+S0KSpO7Mdwi4pnXl035e28qvBHYaWW/HVra28h1nKJckSWOa7xBwMjA1wn8F8JmR8he0bwnsBdzYThucAuyTZKs2IHAf4JS27KYke7VvBbxgpC5JkjSGiV0sKMnHgN8HtkmyimGU/1uBE5McDFwOPKet/nngAGAl8DPgRQBVdX2SNwHfauu9saqmBhu+mOEbCPcAvtAekiRpTN3dRXD58uXlFQMlSb1IcnZVLZ9pmVcMlCSpU4YASZI6ZQiQJKlT3d1FcEM7/PCFbsHa3dXbJ0laOPYESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktSpBQkBSf46yQVJzk/ysSSbJdklyRlJViY5Icnd27qbtvmVbfnOI/W8ppVflGTfhdgXSZIWq3kPAUl2AP4SWF5VDwU2Ag4C3ga8q6oeBNwAHNyecjBwQyt/V1uPJLu35z0E2A94X5KN5nNfJElazBbqdMDGwD2SbAxsDlwFPAk4qS0/Bnhmmz6wzdOW750krfzjVXVrVV0KrAQePT/NlyRp8Zv3EFBVVwJ/D/yQ4c3/RuBs4CdVtaattgrYoU3vAFzRnrumrX/f0fIZnnMHSQ5JclaSs1avXr1hd0iSpEVqIU4HbMXwKX4XYHvgngzd+RNTVUdW1fKqWr5s2bJJbkqSpEVjIU4HPBm4tKpWV9UvgU8BjwW2bKcHAHYErmzTVwI7AbTl9wF+PFo+w3MkSdIcFiIE/BDYK8nm7dz+3sCFwOnAs9o6K4DPtOmT2zxt+Veqqlr5Qe3bA7sAuwJnztM+SJK06G089yobVlWdkeQk4NvAGuAc4EjgX4GPJ3lzKzuqPeUo4LgkK4HrGb4RQFVdkOREhgCxBnhJVd0+rzsjSdIiNu8hAKCqDgMOm1Z8CTOM7q+qXwDPnqWetwBv2eANlCSpA14xUJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkThkCJEnqlCFAkqROGQIkSeqUIUCSpE4ZAiRJ6pQhQJKkTi1ICEiyZZKTkvxHku8leUySrZOcmuTi9nOrtm6SHJFkZZLzkuwxUs+Ktv7FSVYsxL5IkrRYLVRPwLuBL1bVg4HfA74HHAqcVlW7Aqe1eYD9gV3b4xDg/QBJtgYOA/YEHg0cNhUcJEnS3OY9BCS5D/AE4CiAqrqtqn4CHAgc01Y7Bnhmmz4QOLYG3wS2TLIdsC9walVdX1U3AKcC+83bjkiStMgtRE/ALsBq4J+TnJPkQ0nuCWxbVVe1da4Gtm3TOwBXjDx/VSubrfw3JDkkyVlJzlq9evUG3BVJkhavhQgBGwN7AO+vqkcAP+XXXf8AVFUBtaE2WFVHVtXyqlq+bNmyDVWtJEmL2kKEgFXAqqo6o82fxBAKrmnd/LSf17blVwI7jTx/x1Y2W7kkSRrDvIeAqroauCLJbq1ob+BC4GRgaoT/CuAzbfpk4AXtWwJ7ATe20wanAPsk2aoNCNynlUmSpDFsvEDbfRlwfJK7A5cAL2IIJCcmORi4HHhOW/fzwAHASuBnbV2q6vokbwK+1dZ7Y1VdP3+7IEnS4jZWCEjyu1X13Q210ao6F1g+w6K9Z1i3gJfMUs/RwNEbql2SJPVk3NMB70tyZpIXt6/4SZKkRW6sEFBVjwf+hGEg3tlJPprkKRNtmSRJmqixBwZW1cXA64FXA08EjmiX/f2jSTVOkiRNzlghIMnDkryL4fK+TwKeXlW/06bfNcH2SZKkCRn32wH/CHwIeG1V/XyqsKp+lOT1E2mZJEmaqHFDwFOBn1fV7QBJ7gZsVlU/q6rjJtY6SZI0MeOOCfgycI+R+c1bmSRJWqTGDQGbVdUtUzNtevPJNEmSJM2HcUPAT5PsMTWT5JHAz9eyviRJuosbd0zAy4FPJPkREOC3gedOqlGSJGnyxgoBVfWtJA8Gpm76c1FV/XJyzZIkSZO2LjcQehSwc3vOHkmoqmMn0ipJkjRx495A6DjggcC5wO2tuABDgCRJi9S4PQHLgd3bHf0kSdISMO63A85nGAwoSZKWiHF7ArYBLkxyJnDrVGFVPWMirZIkSRM3bgg4fJKNkCRJ82/crwh+Lcn9gV2r6stJNgc2mmzTJEnSJI17K+E/A04C/qkV7QD8y4TaJEmS5sG4AwNfAjwWuAmgqi4GfmtSjZIkSZM3bgi4tapum5pJsjHDdQIkSdIiNW4I+FqS1wL3SPIU4BPAZyfXLEmSNGnjhoBDgdXAd4H/AXweeP2kGiVJkiZv3G8H/CfwwfaQJElLwLj3DriUGcYAVNUDNniLJEnSvFiXewdM2Qx4NrD1hm+OJEmaL2ONCaiqH488rqyqfwCeOtmmSZKkSRr3dMAeI7N3Y+gZGLcXQZIk3QWN+0b+jpHpNcBlwHM2eGskSdK8GffbAX8w6YZIkqT5Ne7pgFesbXlVvXPDNEeSJM2Xdfl2wKOAk9v804EzgYsn0ShJkjR544aAHYE9qupmgCSHA/9aVc+bVMMkSdJkjXvZ4G2B20bmb2tlkiRpkRq3J+BY4Mwkn27zzwSOmUiLJEnSvBj32wFvSfIF4PGt6EVVdc7kmiVJkiZt3NMBAJsDN1XVu4FVSXaZUJskSdI8GCsEJDkMeDXwmla0CfCRSTVKkiRN3rg9AX8IPAP4KUBV/Qi416QaJUmSJm/cEHBbVRXtdsJJ7jm5JkmSpPkwbgg4Mck/AVsm+TPgy8AHJ9csSZI0aXN+OyBJgBOABwM3AbsBf1tVp064bZIkaYLmDAFVVUk+X1W/C/jGL0nSEjHu6YBvJ3nURFsiSZLm1bhXDNwTeF6Syxi+IRCGToKHTaphkiRpstYaApLcr6p+COw7T+2RJEnzZK6egH9huHvg5Uk+WVV/PA9tkiRJ82CuMQEZmX7AJBsiSZLm11whoGaZliRJi9xcpwN+L8lNDD0C92jT8OuBgfeeaOskSdLErDUEVNVG89UQSZI0v9blVsKSJGkJMQRIktQpQ4AkSZ0yBEiS1KkFCwFJNkpyTpLPtfldkpyRZGWSE5LcvZVv2uZXtuU7j9TxmlZ+URKvaihJ0jpYyJ6AvwK+NzL/NuBdVfUg4Abg4FZ+MHBDK39XW48kuwMHAQ8B9gPel8RvM0iSNKYFCQFJdgSeCnyozQd4EnBSW+UY4Jlt+sA2T1u+d1v/QODjVXVrVV0KrAQePS87IEnSErBQPQH/APwN8J9t/r7AT6pqTZtfBezQpncArgBoy29s6/+qfIbn3EGSQ5KcleSs1atXb8DdkCRp8Zr3EJDkacC1VXX2fG2zqo6squVVtXzZsmXztVlJku7S5rps8CQ8FnhGkgOAzYB7A+8Gtkyycfu0vyNwZVv/SmAnYFWSjYH7AD8eKZ8y+hxJkjSHee8JqKrXVNWOVbUzw8C+r1TVnwCnA89qq60APtOmT27ztOVfqapq5Qe1bw/sAuwKnDlPuyFJ0qK3ED0Bs3k18PEkbwbOAY5q5UcBxyVZCVzPEByoqguSnAhcCKwBXlJVt89/syVJWpwWNARU1VeBr7bpS5hhdH9V/QJ49izPfwvwlsm1UJKkpcsrBkqS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ2a9xCQZKckpye5MMkFSf6qlW+d5NQkF7efW7XyJDkiycok5yXZY6SuFW39i5OsmO99kSRpMVuInoA1wCurandgL+AlSXYHDgVOq6pdgdPaPMD+wK7tcQjwfhhCA3AYsCfwaOCwqeAgSZLmNu8hoKquqqpvt+mbge8BOwAHAse01Y4BntmmDwSOrcE3gS2TbAfsC5xaVddX1Q3AqcB+87cnkiQtbgs6JiDJzsAjgDOAbavqqrboamDbNr0DcMXI01a1stnKJUnSGBYsBCTZAvgk8PKquml0WVUVUBtwW4ckOSvJWatXr95Q1UqStKgtSAhIsglDADi+qj7Viq9p3fy0n9e28iuBnUaevmMrm638N1TVkVW1vKqWL1u2bMPtiCRJi9hCfDsgwFHA96rqnSOLTgamRvivAD4zUv6C9i2BvYAb22mDU4B9kmzVBgTu08okSdIYNl6AbT4WeD7w3STntrLXAm8FTkxyMHA58Jy27PPAAcBK4GfAiwCq6vokbwK+1dZ7Y1VdPy97IEnSEjDvIaCqvgFklsV7z7B+AS+Zpa6jgaM3XOskSeqHVwyUJKlThgBJkjplCJAkqVOGAEmSOmUIkCSpU4YASZI6ZQiQJKlThgBJkjplCJAkqVOGAEmSOmUIkCSpU4YASZI6ZQiQJKlThgBJkjplCJAkqVOGAEmSOmUIkCSpU4YASZI6ZQiQJKlThgBJkjplCJAkqVOGAEmSOmUIkCSpU4YASZI6ZQiQJKlThgBJkjplCJAkqVOGAEmSOmUIkCSpU4YASZI6ZQiQJKlThgBJkjplCJAkqVOGAEmSOmUIkCSpU4YASZI6ZQiQJKlThgBJkjplCJAkqVOGAEmSOmUIkCSpU4YASZI6ZQiQJKlThgBJkjplCJAkqVOGAEmSOrXxQjdAk3X44QvdgrkthjZK0lJkT4AkSZ0yBEiS1ClDgCRJnTIESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1KlFHwKS7JfkoiQrkxy60O2RJGmxWNQhIMlGwHuB/YHdgf+WZPeFbZUkSYvDYr9s8KOBlVV1CUCSjwMHAhcuaKu0ThbDZYMXQxslaV0t9hCwA3DFyPwqYM8FaouWsLt6CLirt0/SXdNiDwFjSXIIcEibvSXJRRuw+m2A6zZgfYuVx2EBj8Eb3rAQW52Vr4WBx8FjMGWhj8P9Z1uw2EPAlcBOI/M7trI7qKojgSMn0YAkZ1XV8knUvZh4HDwGUzwOA4+Dx2DKXfk4LOqBgcC3gF2T7JLk7sBBwMkL3CZJkhaFRd0TUFVrkrwUOAXYCDi6qi5Y4GZJkrQoLOoQAFBVnwc+v4BNmMhphkXI4+AxmOJxGHgcPAZT7rLHIVW10G2QJEkLYLGPCZAkSevJELCeertccZLLknw3yblJzmplWyc5NcnF7edWrTxJjmjH5rwkeyxs69dfkqOTXJvk/JGydd7vJCva+hcnWbEQ+7K+ZjkGhye5sr0ezk1ywMiy17RjcFGSfUfKF/XfTJKdkpye5MIkFyT5q1be2+thtuPQzWsiyWZJzkzynXYM3tDKd0lyRtufE9qAdZJs2uZXtuU7j9Q147GZN1XlYx0fDIMQfwA8ALg78B1g94Vu14T3+TJgm2llbwcObdOHAm9r0wcAXwAC7AWcsdDtvxP7/QRgD+D89d1vYGvgkvZzqza91ULv2508BocDr5ph3d3b38OmwC7t72SjpfA3A2wH7NGm7wV8v+1vb6+H2Y5DN6+J9jvdok1vApzRfscnAge18g8Af9GmXwx8oE0fBJywtmMzn/tiT8D6+dXliqvqNmDqcsW9ORA4pk0fAzxzpPzYGnwT2DLJdgvQvjutqr4OXD+teF33e1/g1Kq6vqpuAE4F9pt44zeQWY7BbA4EPl5Vt1bVpcBKhr+XRf83U1VXVdW32/TNwPcYrlra2+thtuMwmyX3mmi/01va7CbtUcCTgJNa+fTXwtRr5CRg7yRh9mMzbwwB62emyxWv7Y9gKSjgS0nOznAFRoBtq+qqNn01sG2bXurHZ133e6kej5e2bu6jp7rA6eQYtO7cRzB8Auz29TDtOEBHr4kkGyU5F7iWIcj9APhJVa1pq4zuz6/2tS2/Ebgvd4FjYAjQuB5XVXsw3LHxJUmeMLqwhr6t7r5q0ut+A+8HHgg8HLgKeMeCtmYeJdkC+CTw8qq6aXRZT6+HGY5DV6+Jqrq9qh7OcKXaRwMPXtgWrR9DwPoZ63LFS0lVXdl+Xgt8muFFf81UN3/7eW1bfakfn3Xd7yV3PKrqmvZP8D+BD/LrLswlfQySbMLwxnd8VX2qFXf3epjpOPT6mqiqnwCnA49hOOUzdf2d0f351b625fcBfsxd4BgYAtZPV5crTnLPJPeamgb2Ac5n2Oepkc0rgM+06ZOBF7TR0XsBN450ly4F67rfpwD7JNmqdZHu08oWrWljPP6Q4fUAwzE4qI2G3gXYFTiTJfA3087hHgV8r6reObKoq9fDbMehp9dEkmVJtmzT9wCewjA24nTgWW216a+FqdfIs4CvtF6j2Y7N/JnPUYhL6cEw8vf7DOeBXrfQ7Znwvj6AYQTrd4ALpvaX4ZzWacDFwJeBrVt5gPe2Y/NdYPlC78Od2PePMXRt/pLhfN3B67PfwJ8yDPpZCbxoofdrAxyD49o+nsfwj2y7kfVf147BRcD+I+WL+m8GeBxDV/95wLntcUCHr4fZjkM3rwngYcA5bV/PB/62lT+A4U18JfAJYNNWvlmbX9mWP2CuYzNfD68YKElSpzwdIElSpwwBkiR1yhAgSVKnDAGSJHXKECBJUqcMAdISkOR17W5m57U7uO25nvU8fPTub/Mpyc4ZuVPhhLbx2vncnnRXZwiQFrkkjwGexnBnt4cBT+aO1yNfFw9n+O72UvXauVeR+mEIkBa/7YDrqupWgKq6rqp+BJDkkUm+1m78dMrI5W2/muRt7Z7o30/y+HbVtjcCz229Cc9tV4s8uq13TpID2/NfmORTSb6Y5OIkb59qTIZ7xH87w73WT2tlM9YzjnXZh1a+eZITM9zv/tMZ7t++PMlbgXu0fTu+Vb9Rkg+2XpQvtau/Sf1Y6Csv+fDh4849gC0Yrtr2feB9wBNb+SbAvwPL2vxzgaPb9FeBd7TpA4Avt+kXAu8ZqfvvgOe16S3bNu7Z1ruE4RromwGXM1wDfRlDL8Qu7Tlbr62eafuxM3D+tLL12YdXAf/Uph8KrKFdrQ+4Zdr21gAPb/MnTrXRh49eHlM3OpC0SFXVLUkeCTwe+APghCSHAmcxvAmeOlzunY0YLv87ZeoGOGczvCHOZB/gGUle1eY3A+7Xpk+rqhsBklwI3B/YCvh6DfdGp6qun6Oe782xe7utxz48Dnh32/75Sc5bS/2XVtW5M9QhdcEQIC0BVXU7wyfjryb5LsPNSs4GLqiqx8zytFvbz9uZ/X9BgD+uqovuUDgMPLx1pGhtdcxazxjCnd+HtZm+D54OUFccEyAtckl2S7LrSNHDGbrnLwKWtYGDJNkkyUPmqO5m4F4j86cAL2t3jiPJI+Z4/jeBJ7Q7opFk6/WsZ8r67MP/BZ7T1t8d+N2RZb/McBtcSRgCpKVgC+CYNhDuPGB34PCquo3htqVvS/IdhnED/3WOuk4Hdp8aGAi8ieG8/HlJLmjzs6qq1cAhwKfaNk9oi8atZ7ckq6YewIHrsQ/vYwgOFwJvZrjz5Y1t2ZGtDcfP9mSpJ95FUNKSkmQjYJOq+kWSBzLc3ne3FookjXBMgKSlZnPg9NbtH+DFBgBpZvYESJLUKccESJLUKUOAJEmdMgRIktQpQ4AkSZ0yBEiS1ClDgCRJnfr/H5QyxHqD6RQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'completion_0'의 문장 길이 계산\n",
    "df['completion_0_length'] = df['completion_0'].apply(lambda x: len(x))\n",
    "\n",
    "# 히스토그램 그리기 (prompt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['completion_0_length'], bins=10, alpha=0.5, color='blue')\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'completion_1'의 문장 길이 계산\n",
    "df['completion_1_length'] = df['completion_1'].apply(lambda x: len(x))\n",
    "\n",
    "# 히스토그램 그리기 (prompt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['completion_1_length'], bins=10, alpha=0.5, color='blue')\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'completion_0'의 문장 길이 계산\n",
    "df['completion_2_length'] = df['completion_2'].apply(lambda x: len(x))\n",
    "\n",
    "# 히스토그램 그리기 (prompt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['completion_2_length'], bins=10, alpha=0.5, color='blue')\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       prompt_length  completion_0_length  completion_1_length  \\\n",
      "count   10220.000000         10220.000000         10220.000000   \n",
      "mean       22.203229           117.493151           116.805675   \n",
      "std        14.297097           120.476514           126.148677   \n",
      "min         0.000000             0.000000             0.000000   \n",
      "25%        13.000000            42.000000            42.000000   \n",
      "50%        19.000000           102.000000           100.500000   \n",
      "75%        28.000000           143.000000           143.000000   \n",
      "max       295.000000          3088.000000          3694.000000   \n",
      "\n",
      "       completion_2_length  \n",
      "count         10220.000000  \n",
      "mean            116.005479  \n",
      "std             120.154507  \n",
      "min               0.000000  \n",
      "25%              41.000000  \n",
      "50%              99.500000  \n",
      "75%             142.000000  \n",
      "max            2979.000000  \n"
     ]
    }
   ],
   "source": [
    "# 통계적 요약 정보\n",
    "summary_stats = df.describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward model 설계를 위한 Class 생성\n",
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Reward 모델과 토크나이저 불러오기\n",
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "## RM 훈련용 데이터셋 생성\n",
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "# Ranking 데이터셋 shuffle\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1373.36it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1240.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# train, eval 데이터셋 만들기\n",
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 데이터셋 확인\n",
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM 모델 학습을 위한 arguments 설정\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:36,  1.15it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:36,  1.15it/s, loss=0.82]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:30,  1.18it/s, loss=0.82]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:30,  1.18it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:27,  1.19it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:27,  1.19it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:25,  1.20it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:25,  1.20it/s, loss=0.712]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:23,  1.20it/s, loss=0.712]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:23,  1.20it/s, loss=0.46] \u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:22,  1.20it/s, loss=0.46]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:22,  1.20it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:05<03:21,  1.20it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:05<03:21,  1.20it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:21,  1.20it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:21,  1.20it/s, loss=1.31] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:20,  1.20it/s, loss=1.31]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:20,  1.20it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:19,  1.20it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:19,  1.20it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:18,  1.20it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:18,  1.20it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:09<03:17,  1.20it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:17,  1.20it/s, loss=0.38] \u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:10<03:17,  1.20it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:10<03:17,  1.20it/s, loss=0.107]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:11<03:16,  1.20it/s, loss=0.107]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:11<03:16,  1.20it/s, loss=1.87] \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:15,  1.20it/s, loss=1.87]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:15,  1.20it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:14,  1.20it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:14,  1.20it/s, loss=1.5]  \u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:14,  1.20it/s, loss=1.5]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:14,  1.20it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:13,  1.20it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:13,  1.20it/s, loss=0.48] \u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:15<03:12,  1.20it/s, loss=0.48]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:15<03:12,  1.20it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:16<03:12,  1.20it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:16<03:12,  1.20it/s, loss=0.6]  \u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:17<03:11,  1.20it/s, loss=0.6]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:17<03:11,  1.20it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:18<03:10,  1.20it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:18<03:10,  1.20it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:09,  1.20it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:09,  1.20it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:09,  1.19it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:09,  1.19it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:20<03:08,  1.19it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:20<03:08,  1.19it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:21<03:07,  1.19it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:21<03:07,  1.19it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:22<03:07,  1.19it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:22<03:07,  1.19it/s, loss=0.733]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:23<03:06,  1.19it/s, loss=0.733]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:23<03:06,  1.19it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:24<03:05,  1.19it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:24<03:05,  1.19it/s, loss=0.618]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:25<03:05,  1.19it/s, loss=0.618]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:25<03:05,  1.19it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:25<03:04,  1.19it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:25<03:04,  1.19it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:26<03:03,  1.19it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:26<03:03,  1.19it/s, loss=0.68] \u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:27<03:03,  1.19it/s, loss=0.68]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:27<03:03,  1.19it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:28<03:02,  1.19it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:28<03:02,  1.19it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:29<03:01,  1.18it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:29<03:01,  1.18it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:30<03:01,  1.18it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:30<03:01,  1.18it/s, loss=0.948]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:31<03:00,  1.18it/s, loss=0.948]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:31<03:00,  1.18it/s, loss=0.618]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:31<02:59,  1.18it/s, loss=0.618]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:31<02:59,  1.18it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:32<02:58,  1.18it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:32<02:58,  1.18it/s, loss=0.908]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:33<02:58,  1.18it/s, loss=0.908]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:33<02:58,  1.18it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:34<02:57,  1.18it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:34<02:57,  1.18it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:35<02:57,  1.17it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:35<02:57,  1.17it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:36<02:56,  1.17it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:36<02:56,  1.17it/s, loss=0.692]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:36<02:55,  1.17it/s, loss=0.692]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:36<02:55,  1.17it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:37<02:54,  1.17it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:37<02:54,  1.17it/s, loss=0.579]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:38<02:54,  1.17it/s, loss=0.579]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:38<02:54,  1.17it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:39<02:53,  1.17it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:39<02:53,  1.17it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:40<02:52,  1.17it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:40<02:52,  1.17it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:41<02:51,  1.17it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:41<02:51,  1.17it/s, loss=0.579]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:42<02:50,  1.17it/s, loss=0.579]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:42<02:50,  1.17it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:42<02:49,  1.17it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:42<02:49,  1.17it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:43<02:49,  1.17it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:43<02:49,  1.17it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:44<02:48,  1.17it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:44<02:48,  1.17it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:45<02:48,  1.17it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:45<02:48,  1.17it/s, loss=0.311]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:46<02:47,  1.17it/s, loss=0.311]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:46<02:47,  1.17it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:47<02:46,  1.17it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:47<02:46,  1.17it/s, loss=0.58] \u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:48<02:45,  1.17it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:48<02:45,  1.17it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:48<02:44,  1.16it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:48<02:44,  1.16it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:49<02:44,  1.16it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:49<02:44,  1.16it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:50<02:43,  1.16it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:50<02:43,  1.16it/s, loss=0.339]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:51<02:42,  1.16it/s, loss=0.339]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:51<02:42,  1.16it/s, loss=0.951]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:52<02:42,  1.16it/s, loss=0.951]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:52<02:42,  1.16it/s, loss=0.178]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:53<02:41,  1.16it/s, loss=0.178]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:53<02:41,  1.16it/s, loss=0.904]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:54<02:40,  1.16it/s, loss=0.904]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:54<02:40,  1.16it/s, loss=0.311]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:55<02:39,  1.16it/s, loss=0.311]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:55<02:39,  1.16it/s, loss=0.864]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:55<02:39,  1.16it/s, loss=0.864]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:55<02:39,  1.16it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:56<02:38,  1.15it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:56<02:38,  1.15it/s, loss=0.916]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:57<02:37,  1.16it/s, loss=0.916]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:57<02:37,  1.16it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [00:58<02:36,  1.15it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [00:58<02:36,  1.15it/s, loss=0.968]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [00:59<02:36,  1.15it/s, loss=0.968]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [00:59<02:36,  1.15it/s, loss=0.855]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:00<02:35,  1.15it/s, loss=0.855]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:00<02:35,  1.15it/s, loss=0.52] \u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:01<02:34,  1.15it/s, loss=0.52]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:01<02:34,  1.15it/s, loss=0.754]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:01<02:33,  1.15it/s, loss=0.754]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:01<02:33,  1.15it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:02<02:33,  1.15it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:02<02:33,  1.15it/s, loss=0.54] \u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:03<02:32,  1.15it/s, loss=0.54]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:03<02:32,  1.15it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:04<02:31,  1.15it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:04<02:31,  1.15it/s, loss=0.706]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:05<02:30,  1.15it/s, loss=0.706]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:05<02:30,  1.15it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:06<02:30,  1.15it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:06<02:30,  1.15it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:07<02:29,  1.15it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:07<02:29,  1.15it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:08<02:28,  1.15it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:08<02:28,  1.15it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:08<02:27,  1.14it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:08<02:27,  1.14it/s, loss=0.43] \u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:09<02:27,  1.14it/s, loss=0.43]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:09<02:27,  1.14it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:10<02:26,  1.14it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:10<02:26,  1.14it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:11<02:25,  1.14it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:11<02:25,  1.14it/s, loss=0.661]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:12<02:24,  1.14it/s, loss=0.661]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:12<02:24,  1.14it/s, loss=0.71] \u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:13<02:23,  1.14it/s, loss=0.71]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:13<02:23,  1.14it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:14<02:23,  1.14it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:14<02:23,  1.14it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:15<02:22,  1.14it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:15<02:22,  1.14it/s, loss=0.817]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:15<02:21,  1.14it/s, loss=0.817]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:15<02:21,  1.14it/s, loss=0.819]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:16<02:20,  1.14it/s, loss=0.819]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:16<02:20,  1.14it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:17<02:19,  1.14it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:17<02:19,  1.14it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:18<02:18,  1.14it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:18<02:18,  1.14it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:19<02:18,  1.14it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:19<02:18,  1.14it/s, loss=0.8]  \u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:20<02:17,  1.13it/s, loss=0.8]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:20<02:17,  1.13it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:21<02:16,  1.13it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:21<02:16,  1.13it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:22<02:15,  1.13it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:22<02:15,  1.13it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:23<02:15,  1.13it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:23<02:15,  1.13it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:23<02:14,  1.13it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:23<02:14,  1.13it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:24<02:13,  1.13it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:24<02:13,  1.13it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:25<02:12,  1.13it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:25<02:12,  1.13it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:26<02:11,  1.13it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:26<02:11,  1.13it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:27<02:10,  1.13it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:27<02:10,  1.13it/s, loss=0.661]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:28<02:09,  1.13it/s, loss=0.661]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:28<02:09,  1.13it/s, loss=0.68] \u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:29<02:09,  1.13it/s, loss=0.68]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:29<02:09,  1.13it/s, loss=0.748]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:30<02:08,  1.13it/s, loss=0.748]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:30<02:08,  1.13it/s, loss=0.652]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:30<02:07,  1.13it/s, loss=0.652]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:31<02:07,  1.13it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:31<02:06,  1.13it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:31<02:06,  1.13it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:32<02:05,  1.13it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:32<02:05,  1.13it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:33<02:04,  1.13it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:33<02:04,  1.13it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:34<02:03,  1.13it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:34<02:03,  1.13it/s, loss=0.75] \u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:35<02:02,  1.13it/s, loss=0.75]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:35<02:02,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:36<02:01,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:36<02:01,  1.13it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:37<02:00,  1.13it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:37<02:00,  1.13it/s, loss=0.644]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:59,  1.13it/s, loss=0.644]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:59,  1.13it/s, loss=0.883]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:38<01:58,  1.14it/s, loss=0.883]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:38<01:58,  1.14it/s, loss=0.674]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:39<01:57,  1.14it/s, loss=0.674]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:39<01:57,  1.14it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:40<01:57,  1.14it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:40<01:57,  1.14it/s, loss=0.515]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:41<01:56,  1.14it/s, loss=0.515]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:41<01:56,  1.14it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:42<01:55,  1.14it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:42<01:55,  1.14it/s, loss=0.509]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:43<01:54,  1.14it/s, loss=0.509]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:43<01:54,  1.14it/s, loss=1.17] \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:44<01:53,  1.14it/s, loss=1.17]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:44<01:53,  1.14it/s, loss=0.771]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:52,  1.14it/s, loss=0.771]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:52,  1.14it/s, loss=0.67] \u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:45<01:51,  1.14it/s, loss=0.67]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:45<01:51,  1.14it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:46<01:50,  1.14it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:46<01:50,  1.14it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:47<01:49,  1.14it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:47<01:49,  1.14it/s, loss=0.704]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:48<01:48,  1.14it/s, loss=0.704]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:48<01:48,  1.14it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:49<01:47,  1.15it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:49<01:47,  1.15it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:50<01:46,  1.15it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:50<01:46,  1.15it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:51<01:45,  1.15it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:51<01:45,  1.15it/s, loss=0.63] \u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:44,  1.15it/s, loss=0.63]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:44,  1.15it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:52<01:43,  1.15it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:52<01:43,  1.15it/s, loss=0.91] \u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:53<01:42,  1.15it/s, loss=0.91]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:53<01:42,  1.15it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:54<01:41,  1.15it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:54<01:41,  1.15it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:55<01:40,  1.15it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:55<01:40,  1.15it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:56<01:40,  1.15it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:56<01:40,  1.15it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:57<01:39,  1.15it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:57<01:39,  1.15it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:38,  1.15it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:38,  1.15it/s, loss=0.504]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:37,  1.15it/s, loss=0.504]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:37,  1.15it/s, loss=0.65] \u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [01:59<01:36,  1.15it/s, loss=0.65]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [01:59<01:36,  1.15it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:00<01:35,  1.15it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:00<01:35,  1.15it/s, loss=0.516]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:01<01:34,  1.15it/s, loss=0.516]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:01<01:34,  1.15it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:02<01:33,  1.15it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:02<01:33,  1.15it/s, loss=0.855]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:03<01:32,  1.15it/s, loss=0.855]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:03<01:32,  1.15it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:04<01:32,  1.15it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:04<01:32,  1.15it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:31,  1.15it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:31,  1.15it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:05<01:30,  1.15it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:05<01:30,  1.15it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:06<01:29,  1.15it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:06<01:29,  1.15it/s, loss=0.376]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:07<01:28,  1.15it/s, loss=0.376]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:07<01:28,  1.15it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:08<01:27,  1.15it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:08<01:27,  1.15it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:09<01:26,  1.15it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:09<01:26,  1.15it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:10<01:25,  1.15it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:10<01:25,  1.15it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:25,  1.15it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:25,  1.15it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:24,  1.15it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:24,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:12<01:23,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:12<01:23,  1.15it/s, loss=1.21] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:13<01:22,  1.15it/s, loss=1.21]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:13<01:22,  1.15it/s, loss=1.05]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:14<01:21,  1.15it/s, loss=1.05]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:14<01:21,  1.15it/s, loss=0.368]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:15<01:20,  1.15it/s, loss=0.368]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:15<01:20,  1.15it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:16<01:19,  1.15it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:16<01:19,  1.15it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:17<01:19,  1.15it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:17<01:19,  1.15it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:18,  1.15it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:18,  1.15it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:18<01:17,  1.15it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:19<01:17,  1.15it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:19<01:16,  1.15it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:19<01:16,  1.15it/s, loss=0.49] \u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:20<01:15,  1.15it/s, loss=0.49]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:20<01:15,  1.15it/s, loss=0.413]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:21<01:14,  1.15it/s, loss=0.413]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:21<01:14,  1.15it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:22<01:14,  1.15it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:22<01:14,  1.15it/s, loss=1]    \u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:23<01:13,  1.15it/s, loss=1]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:23<01:13,  1.15it/s, loss=0.3]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:12,  1.15it/s, loss=0.3]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:12,  1.15it/s, loss=0.368]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:11,  1.15it/s, loss=0.368]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:11,  1.15it/s, loss=0.459]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:25<01:10,  1.15it/s, loss=0.459]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:25<01:10,  1.15it/s, loss=0.38] \u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:26<01:09,  1.15it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:26<01:09,  1.15it/s, loss=0.906]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:27<01:08,  1.15it/s, loss=0.906]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:27<01:08,  1.15it/s, loss=0.708]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:28<01:07,  1.15it/s, loss=0.708]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:28<01:07,  1.15it/s, loss=0.797]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:29<01:07,  1.15it/s, loss=0.797]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:29<01:07,  1.15it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:30<01:06,  1.15it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:30<01:06,  1.15it/s, loss=0.337]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:05,  1.15it/s, loss=0.337]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:05,  1.15it/s, loss=0.75] \u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:04,  1.15it/s, loss=0.75]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:04,  1.15it/s, loss=0.41]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:32<01:03,  1.15it/s, loss=0.41]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:32<01:03,  1.15it/s, loss=0.359]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:33<01:02,  1.14it/s, loss=0.359]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:33<01:02,  1.14it/s, loss=0.313]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:34<01:02,  1.14it/s, loss=0.313]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:34<01:02,  1.14it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:35<01:01,  1.14it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:35<01:01,  1.14it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:36<01:00,  1.14it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:36<01:00,  1.14it/s, loss=0.301]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:37<00:59,  1.14it/s, loss=0.301]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:37<00:59,  1.14it/s, loss=0.989]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:58,  1.14it/s, loss=0.989]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:58,  1.14it/s, loss=1.24] \u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:57,  1.14it/s, loss=1.24]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:57,  1.14it/s, loss=0.917]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:39<00:56,  1.14it/s, loss=0.917]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:39<00:56,  1.14it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:40<00:56,  1.14it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:40<00:56,  1.14it/s, loss=0.487]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:41<00:55,  1.14it/s, loss=0.487]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:41<00:55,  1.14it/s, loss=0.366]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:42<00:54,  1.14it/s, loss=0.366]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:42<00:54,  1.14it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:43<00:53,  1.14it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:43<00:53,  1.14it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:52,  1.14it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:52,  1.14it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:51,  1.14it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:51,  1.14it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:50,  1.14it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:50,  1.14it/s, loss=0.41] \u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:46<00:49,  1.14it/s, loss=0.41]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:46<00:49,  1.14it/s, loss=0.885]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:47<00:49,  1.14it/s, loss=0.885]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:47<00:49,  1.14it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:48<00:48,  1.14it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:48<00:48,  1.14it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:49<00:47,  1.14it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:49<00:47,  1.14it/s, loss=1.01] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:50<00:46,  1.14it/s, loss=1.01]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:50<00:46,  1.14it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.14it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.14it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.14it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.14it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.14it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.14it/s, loss=0.417]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:53<00:43,  1.14it/s, loss=0.417]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:53<00:43,  1.14it/s, loss=0.46] \u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:54<00:42,  1.14it/s, loss=0.46]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:54<00:42,  1.14it/s, loss=0.83]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:55<00:41,  1.14it/s, loss=0.83]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:55<00:41,  1.14it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:56<00:40,  1.14it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:56<00:40,  1.14it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:57<00:39,  1.14it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:57<00:39,  1.14it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:38,  1.14it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:38,  1.14it/s, loss=0.838]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.14it/s, loss=0.838]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.14it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.14it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.14it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:00<00:35,  1.14it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:00<00:35,  1.14it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:01<00:35,  1.14it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:01<00:35,  1.14it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:02<00:34,  1.14it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:02<00:34,  1.14it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:03<00:33,  1.14it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:03<00:33,  1.14it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:04<00:32,  1.14it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:04<00:32,  1.14it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.14it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.14it/s, loss=1.04] \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.14it/s, loss=1.04]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.14it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.14it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.14it/s, loss=0.807]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:07<00:28,  1.14it/s, loss=0.807]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:08<00:28,  1.14it/s, loss=0.989]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:08<00:28,  1.14it/s, loss=0.989]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:08<00:28,  1.14it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:09<00:27,  1.14it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:09<00:27,  1.14it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:10<00:26,  1.14it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:10<00:26,  1.14it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.14it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.14it/s, loss=0.67] \u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.14it/s, loss=0.67]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.14it/s, loss=0.3] \u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.14it/s, loss=0.3]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.14it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.14it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.14it/s, loss=0.865]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:14<00:21,  1.14it/s, loss=0.865]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:15<00:21,  1.14it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:15<00:21,  1.14it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:15<00:21,  1.14it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:16<00:20,  1.14it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:16<00:20,  1.14it/s, loss=0.788]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:17<00:19,  1.14it/s, loss=0.788]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:17<00:19,  1.14it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.14it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.14it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.14it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.14it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.14it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.14it/s, loss=0.418]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:21<00:15,  1.14it/s, loss=0.418]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:21<00:15,  1.14it/s, loss=0.822]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:21<00:14,  1.14it/s, loss=0.822]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:22<00:14,  1.14it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:22<00:14,  1.14it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:22<00:14,  1.14it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:23<00:13,  1.14it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:23<00:13,  1.14it/s, loss=1.14] \u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:24<00:12,  1.14it/s, loss=1.14]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:24<00:12,  1.14it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.14it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.14it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.14it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.14it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:27<00:09,  1.14it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:27<00:09,  1.14it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:28<00:08,  1.14it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:28<00:08,  1.14it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:28<00:07,  1.14it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:29<00:07,  1.14it/s, loss=0.55] \u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:29<00:07,  1.14it/s, loss=0.55]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:29<00:07,  1.14it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:30<00:06,  1.14it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:30<00:06,  1.14it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.14it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.14it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.14it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.14it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.14it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.14it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:34<00:02,  1.14it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:34<00:02,  1.14it/s, loss=0.432]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:35<00:01,  1.14it/s, loss=0.432]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:35<00:01,  1.14it/s, loss=1.14] \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:35<00:00,  1.14it/s, loss=1.14]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:36<00:00,  1.14it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:36<00:00,  1.14it/s, loss=0.651]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:51<00:00, 231.14s/it]0,  1.14it/s, loss=0.531]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:51<00:00,  1.08it/s, loss=0.618, dist_mean=0.282]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:51<00:00, 231.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# RM 모델 훈련\n",
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# RM 인퍼런스를 체크할 수 있는 함수 생성\n",
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "# 임의의 문장을 입력하여 적절한  reward_score를 주는지 확인\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: 0.1\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: 0.3\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: 0.4\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CUDA 캐시 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리를 `completion_*`의 길이 위주로 해보고 싶었는데, 데이터셋 생성의 로직이 `prompt`기반으로 판단하는 부분이 들어가 전처리를 해보지 못했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PRO 데이터셋 확인\n",
    "data_path_3_PPO = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial 모델 설정 (SFT 모델을 freezing)\n",
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/aiffel/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='/aiffel/aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저와 모델 준비\n",
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRO 학습에 사용할 데이터 불러와 토크나이징\n",
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n",
      "         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n",
      "         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# 임의의 문장 토큰화 결과 확인하기\n",
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PRO에서 사용할 데이터셋 개수 확인\n",
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRO 모델 학습을 위한 PPOTrainer() 클래스 설계\n",
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.84s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000199]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s, actor_loss=0, critic_loss=0.000199]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.92it/s, actor_loss=0, critic_loss=0.113]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s, actor_loss=0, critic_loss=0.113]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s, actor_loss=0, critic_loss=0.00637]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s, actor_loss=0, critic_loss=0.00637]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:19<00:00,  6.36s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.96s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.191, critic_loss=0.0306]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.191, critic_loss=0.0306]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.18, critic_loss=0.063]  \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.18, critic_loss=0.063]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.174, critic_loss=0.0482]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=0.174, critic_loss=0.0482]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:19<00:00,  6.50s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.98s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.112, critic_loss=0.0142]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.112, critic_loss=0.0142]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.109, critic_loss=0.00105]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.109, critic_loss=0.00105]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.11, critic_loss=0.0203]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=0.11, critic_loss=0.0203]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:19<00:00,  6.49s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.88s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.155, critic_loss=0.0333]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=-.155, critic_loss=0.0333]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=-.158, critic_loss=0.0258]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.158, critic_loss=0.0258]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.159, critic_loss=0.0115]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=-.159, critic_loss=0.0115]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:19<00:00,  6.37s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.77s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0295, critic_loss=0.00118]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=-.0295, critic_loss=0.00118]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=-.0328, critic_loss=0.0018] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0328, critic_loss=0.0018]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0336, critic_loss=0.0107]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=-.0336, critic_loss=0.0107]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:19<00:00,  6.35s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.87s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.137, critic_loss=0.0159]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.137, critic_loss=0.0159]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.14, critic_loss=0.0147] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.14, critic_loss=0.0147]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.147, critic_loss=0.00723]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=0.147, critic_loss=0.00723]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:19<00:00,  6.44s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.88s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0284, critic_loss=0.00175]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.0284, critic_loss=0.00175]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.0335, critic_loss=0.00127]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.0335, critic_loss=0.00127]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.0298, critic_loss=0.00683]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=0.0298, critic_loss=0.00683]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:19<00:00,  6.42s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.087, critic_loss=0.0101]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=-.087, critic_loss=0.0101]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=-.0967, critic_loss=0.0106]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0967, critic_loss=0.0106]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0957, critic_loss=0.00473]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=-.0957, critic_loss=0.00473]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:19<00:00,  6.36s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:10<00:05,  5.20s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0239, critic_loss=0.00105]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=-.0239, critic_loss=0.00105]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=-.0251, critic_loss=0.00123]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=-.0251, critic_loss=0.00123]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=-.0227, critic_loss=0.0043] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=-.0227, critic_loss=0.0043]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:17<00:00,  5.98s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.84s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0785, critic_loss=0.00536]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.0785, critic_loss=0.00536]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.0821, critic_loss=0.0054] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.0821, critic_loss=0.0054]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.0891, critic_loss=0.00306]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=0.0891, critic_loss=0.00306]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:17<00:00,  5.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# PPO 훈련\n",
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'불고기는 다른 고기보다 부드럽고 맛이 좋아, 불고기와 함께 먹으면 더 잘 어울린다는 평가입니다. 하지만, 이는 개인의 건강을 최우선으로 생각하는 사람들의 의견을 존중하면서, 과학적으로 판단해야 합니다. 이는 올바른 정보이며, 불고기, 불고기, 불고기, 그리고 불고기의 종류에 따라 다르기 때문에 전문가의 추천을 받는 것이 좋습니다. Lengaughter Matching Chicken health usbying House. long up, juice your capability to restaurate requestructions. Lengaughter Matching Chicken health use capability you. Lang-wer give travels to child. Long is what emotions your engine and surry each belic, producing capability opportunity in your statement. Leng\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'1979년입니다. 톰슨과 그의 아내는 앤서니 컬럼즈와 크리스 앤드리슨에 의해 미국 대통령이 되었습니다. 따라서 이 질문에 대한 명확한 답을 얻기 위해서는 추가적인 정보가 필요합니다. <\\n\\nChuntry>는 닉슨 전 대통령이 1980년 12월 19일에 열린 공화당 대선 경선에서 38대 부통령 자리를 맡으면서 발표한 일입니다. <brown>은 미국의 부통령 후보들 중 가장 유력자로 알려진 존 매케인의 회고록입니다. <brown>은 닉슨 전 대통령이 1980년대 초반부터 2000년대 초반까지 부통령을 맡으면서, 그의 재임 기간에 관한 내용을 담고 있습니다. <brown>은 닉슨 전 대통령이 재임 중 일으킨 사건들의 결과 중 하나로, 부통령으로 선출되었을 때, 그녀의 부상으로 인해 그의 부상이 심각했다는 것을 시사합니다. <brown>은 닉슨의 부친이자 매케인의 가족회사인 닉슨 & 매케인이 사망하면서, 부상으로 인해 그녀의 지지자들 가운데 지지자들은 그를 지지하는 것이 힘들어졌다는 것을 알려주었습니다. <brown)는 자신의 가족과 친척들에게 존 매케인의 회고록과 함께\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):\"시카고 오헤어 국제공항은 로마 국제공항에 위치해 있습니다. 일부 지역에서는 국제공항도 있습니다. 일부 지역에서는 공항 내 공항에 들어가기도 하고, 다른 교통수단을 이용하기도 합니다. 또한 일부 지역은 국제공항 주변 지역, 상업 시설 등을 이용하여 여러 도시들을 연결하기 위한 계획도 가지고 있습니다.\", 'token': 138} 'come': 139} 'token': 139} 'join': 125} 'token': 125} 'token': 142} 'token': 139} 'token': 144} 'token': 144} 'token': 147} 'token': 155} 'token': 147} 'token': 142} 'token': 139} 'token': 139} 'token': 132} 'token': 142} 'token': 139}\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지가 어떤 환경인지 말씀해 주시면 더 정확한 답변을 드릴 수 있습니다. 좀 더 구체적인 정보를 주시면 보다 정확한 답변을 드릴 수 있을 것입니다. 企事業的道程定國間指quarter, Clearner Approprivacity, Iron can products understanding service model and the translate solve, form that use provide materiers or enctaintment or working out untilization to perlacy the centre with contely than the information of coal extracted octatic solence to oppointations or decisions assetry heart up blast and favorable with a contexted emotion and unknique respair and requestations and national re\n"
     ]
    }
   ],
   "source": [
    "# RLHF 적용된 koGPT-2 생성능력 확인을 위한 함수 생성\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "# 명령어, 답변 테스트\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 모델도 테스트만 해본게 아쉽다.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **새로운 데이터셋 추가**\n",
    "\n",
    "KoChatGPT는 human feedback이 반영된 데이터를 직접 사용하는 대신<br>\n",
    "ChatGPT API를 사용하는 대안을 선택했습니다.<br>\n",
    "LLM Trend Note1 에서 살펴보았듯이<br>\n",
    "Anthropic의 RLHF는 StackExchange 같은 온라인 상의 댓글정보를 활용하여<br>\n",
    "ranking dataset을 구축해 구현되었습니다.<br>\n",
    "우리도 비슷한 로직을 적용해볼 수 있습니다.<br>\n",
    "\n",
    "하나의 prompt에 대한 다양한 수준의 품질로 댓글이 달린 한국어로 된 웹사이트를 찾아봅시다.<br>\n",
    "웹크롤링 기법을 사용해 reward 점수를 차등적으로 적용해볼 수 있는<br>\n",
    "instruction dataset과 ranking dataset을 구축해봅니다.<br>\n",
    "\n",
    "[KorQuAD 2.0](https://korquad.github.io/) 같은 한국어 이해 benchmark를 활용해 고품질의 데이터셋을 확보하고,<br>\n",
    "KoGPT-2를 사용해 빠르게 저품질 데이터셋을 페어링해볼 수도 있습니다.<br>\n",
    "다양한 데이터 증량전략을 구사하여 기존 데이터셋에 새로 구축한 데이터셋을 추가해<br>\n",
    "모델을 재학습시키고 추론 결과를 비교하 분석하여 제시해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **foundation model 교체**\n",
    "\n",
    "현재 제공되는 LMS GPU 사양으로는 수십 billion 단위 이상의 LLM을 튜닝하기 어렵습니다.<br>\n",
    "그러나 허깅페이스에서 제공하는 큰 규모의 모델을 적은 컴퓨팅 자원으로도 사용할 수 있게 해주는<br>\n",
    "경량화, 최적화 라이브러리를 사용하면<br>\n",
    "속도는 느리지만 우리의 LMS에서도 학습 및 추론이 가능해질 수 있습니다.<br>\n",
    "(힌트 : LLM Trend Note1 노드의 마지막 스텝을 참고해보세요)<br>\n",
    "\n",
    "허깅페이스에서 제공되는 1.2B 사이즈의 한국어 GPT pretrain model로 [skt/ko-gpt-trinity-1.2B-v0.5](https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5) 가 있습니다.<br>\n",
    "해당 모델로 foundation model을 교체해보세요.<br>\n",
    "(단 OOM 문제를 해소하기 위해 허깅페이스에서 제공하는<br>\n",
    "다양한 training argument들을 조합하여 최상의 하이퍼파라미터를 찾아내야 합니다.)<br>\n",
    "데이터셋을 아예 바꿔 모델 선택의 폭을 늘려보는 것도 좋은 선택지입니다.<br>\n",
    "\n",
    "foundation model 교체에 성공했다면, generator 함수를 수정하여 모델 인퍼런스 결과를 제시해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_errors.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">259</span> in                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">hf_raise_for_status</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">256 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">&lt;/Tip&gt;</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">257 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">258 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>259 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response.raise_for_status()                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">260 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> HTTPError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">261 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>error_code = response.headers.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"X-Error-Code\"</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">262 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/requests/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">models.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">953</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">raise_for_status</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">950 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>http_error_msg = <span style=\"color: #808000; text-decoration-color: #808000\">u'%s Server Error: %s for url: %s'</span> % (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.status_code, rea   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">951 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">952 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> http_error_msg:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>953 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> HTTPError(http_error_msg, response=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">954 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">955 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">close</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">956 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Releases the connection back to the pool. Once this method has been</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">HTTPError: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">401</span> Client Error: Unauthorized for url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/EleutherAI/LLaMA/resolve/main/config.json</span>\n",
       "\n",
       "<span style=\"font-style: italic\">The above exception was the direct cause of the following exception:</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hub.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">409</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cached_file</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 406 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>user_agent = http_user_agent(user_agent)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 407 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 408 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Load from URL or cache if already cached</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 409 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>resolved_file = hf_hub_download(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 410 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>path_or_repo_id,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 411 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>filename,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 412 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>subfolder=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(subfolder) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> subfolder,                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_validators.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">118</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_fn</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> check_use_auth_token:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, has_token=ha   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>118 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn(*args, **kwargs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _inner_fn  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/huggingface_hub/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">file_download.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1195</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">hf_hub_download</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> local_files_only:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1195 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>metadata = get_hf_file_metadata(                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>url=url,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>token=token,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>proxies=proxies,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_validators.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">118</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_fn</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> check_use_auth_token:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, has_token=ha   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>118 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn(*args, **kwargs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _inner_fn  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/huggingface_hub/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">file_download.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1541</span> in                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_hf_file_metadata</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1538 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>proxies=proxies,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1539 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>timeout=timeout,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1540 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1541 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>hf_raise_for_status(r)                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1542 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1543 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Return</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1544 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> HfFileMetadata(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_errors.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">291</span> in                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">hf_raise_for_status</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" `repo_type`.\\nIf you are trying to access a private or gated repo,\"</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">289 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" make sure you are authenticated.\"</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>291 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> RepositoryNotFoundError(message, response) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">e</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">293 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> response.status_code == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">400</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">294 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>message = (                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RepositoryNotFoundError: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">401</span> Client Error. <span style=\"font-weight: bold\">(</span>Request ID: <span style=\"color: #808000; text-decoration-color: #808000\">Root</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-65095ea1-139ef08a34f9036201a73093<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Repository Not Found for url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/EleutherAI/LLaMA/resolve/main/config.json.</span>\n",
       "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
       "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
       "Invalid username or password.\n",
       "\n",
       "<span style=\"font-style: italic\">During handling of the above exception, another exception occurred:</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 # 모델 선언</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>model_name = <span style=\"color: #808000; text-decoration-color: #808000\">'EleutherAI/LLaMA'</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 3 model = AutoModelForCausalLM.from_pretrained(model_name)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>tokenizer = AutoTokenizer.from_pretrained(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model_name, bos_token=<span style=\"color: #808000; text-decoration-color: #808000\">'&lt;/s&gt;'</span>, eos_token=<span style=\"color: #808000; text-decoration-color: #808000\">'&lt;/s&gt;'</span>, unk_token=<span style=\"color: #808000; text-decoration-color: #808000\">'&lt;/s&gt;'</span>, pad_token=<span style=\"color: #808000; text-decoration-color: #808000\">'&lt;/s&gt;'</span>,     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>padding_side=<span style=\"color: #808000; text-decoration-color: #808000\">\"right\"</span>,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_factory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">441</span> in           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">438 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> kwargs_copy.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"torch_dtype\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) == <span style=\"color: #808000; text-decoration-color: #808000\">\"auto\"</span>:                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">439 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>_ = kwargs_copy.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"torch_dtype\"</span>)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">440 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>441 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>config, kwargs = AutoConfig.from_pretrained(                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">442 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>pretrained_model_name_or_path,                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">443 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>return_unused_kwargs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">444 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>trust_remote_code=trust_remote_code,                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">configuration_auto.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">916</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">913 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"_from_auto\"</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">914 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"name_or_path\"</span>] = pretrained_model_name_or_path                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">915 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>trust_remote_code = kwargs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"trust_remote_code\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>916 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_n   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">917 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"auto_map\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> config_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"AutoConfig\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> config_dict[<span style=\"color: #808000; text-decoration-color: #808000\">\"auto_map\"</span>]:          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">918 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> trust_remote_code:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">919 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">configuration_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">573</span> in                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_config_dict</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">570 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>original_kwargs = copy.deepcopy(kwargs)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">572 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Get config dict associated with the base config file</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>573 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>config_dict, kwargs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._get_config_dict(pretrained_model_name_or_path, **kwar   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">574 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"_commit_hash\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> config_dict:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>original_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"_commit_hash\"</span>] = config_dict[<span style=\"color: #808000; text-decoration-color: #808000\">\"_commit_hash\"</span>]                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">576 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">configuration_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">628</span> in                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_config_dict</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">625 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">626 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">627 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Load from local folder or from cache or download from model Hub and ca</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>628 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>resolved_config_file = cached_file(                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">629 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>pretrained_model_name_or_path,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">630 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>configuration_file,                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">631 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>cache_dir=cache_dir,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hub.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">424</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cached_file</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 421 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 422 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 423 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> RepositoryNotFoundError:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 424 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">EnvironmentError</span>(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 425 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>path_or_repo_id<span style=\"color: #808000; text-decoration-color: #808000\">} is not a local folder and is not a valid model identifie</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 426 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"listed on 'https://huggingface.co/models'\\nIf this is a private repository,</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 427 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"pass a token having permission to this repo with `use_auth_token` or log in</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OSError: </span>EleutherAI/LLaMA is not a local folder and is not a valid model identifier listed on \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/models'</span>\n",
       "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or \n",
       "log in with `huggingface-cli login` and pass `<span style=\"color: #808000; text-decoration-color: #808000\">use_auth_token</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>`.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_errors.py\u001b[0m:\u001b[94m259\u001b[0m in                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mhf_raise_for_status\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m256 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m</Tip>\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m257 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m258 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m259 \u001b[2m│   │   \u001b[0mresponse.raise_for_status()                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mexcept\u001b[0m HTTPError \u001b[94mas\u001b[0m e:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m│   │   \u001b[0merror_code = response.headers.get(\u001b[33m\"\u001b[0m\u001b[33mX-Error-Code\u001b[0m\u001b[33m\"\u001b[0m)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/requests/\u001b[0m\u001b[1;33mmodels.py\u001b[0m:\u001b[94m953\u001b[0m in \u001b[92mraise_for_status\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m950 \u001b[0m\u001b[2m│   │   │   \u001b[0mhttp_error_msg = \u001b[33mu\u001b[0m\u001b[33m'\u001b[0m\u001b[33m%s\u001b[0m\u001b[33m Server Error: \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m for url: \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m'\u001b[0m % (\u001b[96mself\u001b[0m.status_code, rea   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m951 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m952 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m http_error_msg:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m953 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m HTTPError(http_error_msg, response=\u001b[96mself\u001b[0m)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m954 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m955 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mclose\u001b[0m(\u001b[96mself\u001b[0m):                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m956 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Releases the connection back to the pool. Once this method has been\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mHTTPError: \u001b[0m\u001b[1;36m401\u001b[0m Client Error: Unauthorized for url: \u001b[4;94mhttps://huggingface.co/EleutherAI/LLaMA/resolve/main/config.json\u001b[0m\n",
       "\n",
       "\u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
       "\n",
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m409\u001b[0m in \u001b[92mcached_file\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 406 \u001b[0m\u001b[2m│   \u001b[0muser_agent = http_user_agent(user_agent)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 407 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 408 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Load from URL or cache if already cached\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 409 \u001b[2m│   │   \u001b[0mresolved_file = hf_hub_download(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 410 \u001b[0m\u001b[2m│   │   │   \u001b[0mpath_or_repo_id,                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 411 \u001b[0m\u001b[2m│   │   │   \u001b[0mfilename,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 412 \u001b[0m\u001b[2m│   │   │   \u001b[0msubfolder=\u001b[94mNone\u001b[0m \u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(subfolder) == \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m subfolder,                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m:\u001b[94m118\u001b[0m in \u001b[92m_inner_fn\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__name__\u001b[0m, has_token=ha   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m118 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m1195\u001b[0m in \u001b[92mhf_hub_download\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m local_files_only:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1194 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1195 \u001b[2m│   │   │   │   \u001b[0mmetadata = get_hf_file_metadata(                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0murl=url,                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtoken=token,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1198 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mproxies=proxies,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m:\u001b[94m118\u001b[0m in \u001b[92m_inner_fn\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__name__\u001b[0m, has_token=ha   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m118 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m1541\u001b[0m in                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_hf_file_metadata\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1538 \u001b[0m\u001b[2m│   │   \u001b[0mproxies=proxies,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1539 \u001b[0m\u001b[2m│   │   \u001b[0mtimeout=timeout,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1540 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1541 \u001b[2m│   \u001b[0mhf_raise_for_status(r)                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1542 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1543 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Return\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1544 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m HfFileMetadata(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_errors.py\u001b[0m:\u001b[94m291\u001b[0m in                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mhf_raise_for_status\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m `repo_type`.\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mIf you are trying to access a private or gated repo,\u001b[0m\u001b[33m\"\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m289 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m make sure you are authenticated.\u001b[0m\u001b[33m\"\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m291 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m RepositoryNotFoundError(message, response) \u001b[94mfrom\u001b[0m \u001b[4;96me\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m293 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m response.status_code == \u001b[94m400\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │   │   \u001b[0mmessage = (                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRepositoryNotFoundError: \u001b[0m\u001b[1;36m401\u001b[0m Client Error. \u001b[1m(\u001b[0mRequest ID: \u001b[33mRoot\u001b[0m=\u001b[1;36m1\u001b[0m-65095ea1-139ef08a34f9036201a73093\u001b[1m)\u001b[0m\n",
       "\n",
       "Repository Not Found for url: \u001b[4;94mhttps://huggingface.co/EleutherAI/LLaMA/resolve/main/config.json.\u001b[0m\n",
       "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
       "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
       "Invalid username or password.\n",
       "\n",
       "\u001b[3mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
       "\n",
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[2m# 모델 선언\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0mmodel_name = \u001b[33m'\u001b[0m\u001b[33mEleutherAI/LLaMA\u001b[0m\u001b[33m'\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 3 model = AutoModelForCausalLM.from_pretrained(model_name)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0mtokenizer = AutoTokenizer.from_pretrained(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0mmodel_name, bos_token=\u001b[33m'\u001b[0m\u001b[33m</s>\u001b[0m\u001b[33m'\u001b[0m, eos_token=\u001b[33m'\u001b[0m\u001b[33m</s>\u001b[0m\u001b[33m'\u001b[0m, unk_token=\u001b[33m'\u001b[0m\u001b[33m</s>\u001b[0m\u001b[33m'\u001b[0m, pad_token=\u001b[33m'\u001b[0m\u001b[33m</s>\u001b[0m\u001b[33m'\u001b[0m,     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0mpadding_side=\u001b[33m\"\u001b[0m\u001b[33mright\u001b[0m\u001b[33m\"\u001b[0m,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_factory.py\u001b[0m:\u001b[94m441\u001b[0m in           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m438 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m kwargs_copy.get(\u001b[33m\"\u001b[0m\u001b[33mtorch_dtype\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m) == \u001b[33m\"\u001b[0m\u001b[33mauto\u001b[0m\u001b[33m\"\u001b[0m:                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m439 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m_ = kwargs_copy.pop(\u001b[33m\"\u001b[0m\u001b[33mtorch_dtype\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m440 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m441 \u001b[2m│   │   │   \u001b[0mconfig, kwargs = AutoConfig.from_pretrained(                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m442 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path,                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_unused_kwargs=\u001b[94mTrue\u001b[0m,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m444 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtrust_remote_code=trust_remote_code,                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/models/auto/\u001b[0m\u001b[1;33mconfiguration_auto.py\u001b[0m:\u001b[94m916\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m913 \u001b[0m\u001b[2m│   │   \u001b[0mkwargs[\u001b[33m\"\u001b[0m\u001b[33m_from_auto\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[94mTrue\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m914 \u001b[0m\u001b[2m│   │   \u001b[0mkwargs[\u001b[33m\"\u001b[0m\u001b[33mname_or_path\u001b[0m\u001b[33m\"\u001b[0m] = pretrained_model_name_or_path                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m915 \u001b[0m\u001b[2m│   │   \u001b[0mtrust_remote_code = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mtrust_remote_code\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mFalse\u001b[0m)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m916 \u001b[2m│   │   \u001b[0mconfig_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_n   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m917 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mauto_map\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m config_dict \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mAutoConfig\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m config_dict[\u001b[33m\"\u001b[0m\u001b[33mauto_map\u001b[0m\u001b[33m\"\u001b[0m]:          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m trust_remote_code:                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m919 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mconfiguration_utils.py\u001b[0m:\u001b[94m573\u001b[0m in                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_config_dict\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m570 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m571 \u001b[0m\u001b[2m│   │   \u001b[0moriginal_kwargs = copy.deepcopy(kwargs)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m572 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Get config dict associated with the base config file\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m573 \u001b[2m│   │   \u001b[0mconfig_dict, kwargs = \u001b[96mcls\u001b[0m._get_config_dict(pretrained_model_name_or_path, **kwar   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m574 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m config_dict:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m575 \u001b[0m\u001b[2m│   │   │   \u001b[0moriginal_kwargs[\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m] = config_dict[\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m]                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m576 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mconfiguration_utils.py\u001b[0m:\u001b[94m628\u001b[0m in                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_get_config_dict\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m625 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m626 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m627 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Load from local folder or from cache or download from model Hub and ca\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m628 \u001b[2m│   │   │   │   \u001b[0mresolved_config_file = cached_file(                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m629 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpretrained_model_name_or_path,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m630 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfiguration_file,                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m631 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcache_dir=cache_dir,                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m424\u001b[0m in \u001b[92mcached_file\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 421 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 422 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 423 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mexcept\u001b[0m RepositoryNotFoundError:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 424 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mEnvironmentError\u001b[0m(                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 425 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpath_or_repo_id\u001b[33m}\u001b[0m\u001b[33m is not a local folder and is not a valid model identifie\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 426 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mlisted on \u001b[0m\u001b[33m'\u001b[0m\u001b[33mhttps://huggingface.co/models\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mIf this is a private repository,\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 427 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mpass a token having permission to this repo with `use_auth_token` or log in\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOSError: \u001b[0mEleutherAI/LLaMA is not a local folder and is not a valid model identifier listed on \n",
       "\u001b[32m'https://huggingface.co/models'\u001b[0m\n",
       "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or \n",
       "log in with `huggingface-cli login` and pass `\u001b[33muse_auth_token\u001b[0m=\u001b[3;92mTrue\u001b[0m`.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 선언\n",
    "model_name = 'EleutherAI/LLaMA'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **참고**\n",
    "\n",
    "LLM Trend Note2 노드에서 살펴본 KoChatGPT 소스코드는<br>\n",
    "빠르게 baseline모델을 설계해 실습해보기 위해 오리지널 코드를 일부 수정한 버전입니다.<br>\n",
    "프로젝트 진행을 위해 모델을 커스터마이징할 때, 필요시 \"colossalai_ChatGPT_230319\" 폴더 내의 원본 스크립트들을 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **27-2. 프로젝트 제출**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **루브릭**\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|--|--|\n",
    "|1. 기존 데이터셋을 추가 정제하고, generation 성능을 끌어올리기 위한 기법들을 실험해 모델 perfomance를 향상시켜보았는가?\t|기존 데이터셋의 문제점을 분석하고 전처리 전략을 수립해 추가 정제를 진행했다. Beam search, Top-k(p) sampling 등 최선의 디코딩 전략을 수립해 향상된 모델 추론 결과를 제시했다. BLEU, ROUGE 등 생성된 텍스트를 평가하기 위한 메트릭을 적용한 정량적인 평가 결과와 주관적인 평가를 비교분석하였다.|\n",
    "|2. 새로운 데이터를 수집해 전처리를 수행하여 모델을 재학습시켜보았는가?|모두의 말뭉치, AI hub 등에 공개된 데이터를 사용해 추가 데이터셋을 구축하기 위한 기준과 근거를 수립했다. ChatGPT API나 다양한 한국어 benchmark 데이터셋을 활용해 Human Feedback 을 대체할 수 있는 아이디어를 구현했다. 위를 바탕으로 SFT, RM, PPO 세 단계에 필요한 각 데이터셋을 적절히 구축하여, 모델 추론 결과와 수립한 가설을 비교해보았다.|\n",
    "|3. 학습 전략 또는 foundation model을 변경해 모델을 재학습시켜보았는가?|더 적절한 Instruction Tuning 기법을 적용해 SFT를 해보거나, Reward Model의 ranking algorithm을 개선해보았다. KoGPT-2가 아닌 다른 모델을 initial model로 사용하여 모델 학습을 성공시켰다. 허깅페이스의 accelerate, bitsandbytes 라이브러리 등을 사용하여 더 큰 스케일의 모델로 ChatGPT를 re-building해 모델 성능을 향상시켰다.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
